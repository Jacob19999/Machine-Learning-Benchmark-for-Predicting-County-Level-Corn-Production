{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f99d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required packages for this notebook (run once per environment)\n",
    "# Tip: In Jupyter, use %pip so installs go to the current kernel.\n",
    "%pip install -q geopandas pandas numpy xarray rioxarray rasterio rasterstats requests tqdm pyproj shapely fiona py7zr nbformat netCDF4 h5netcdf pydap cftime\n",
    "\n",
    "# Optional performance/feature extras:\n",
    "# %pip install -q aiohttp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa8e981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRISM_SOURCE = CSV_ONLY\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === DATA SOURCE CONFIG ===\n",
    "PRISM_SOURCE = \"CSV_ONLY\"  # enforce CSV-only PRISM\n",
    "print(\"PRISM_SOURCE =\", PRISM_SOURCE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a24b5d5",
   "metadata": {},
   "source": [
    "\n",
    "# Minnesota Corn ‚Äî Minimal Schema Builder (May/Jul/Aug horizons)\n",
    "\n",
    "This notebook downloads and assembles external datasets into the **minimal schema** we discussed for county-level corn modeling in Minnesota.\n",
    "\n",
    "**What you get**\n",
    "- County-month tables keyed by `fips, date, horizon_tag` with PRISM, GLDAS, NDVI, drought, ONI, soils, CDL corn fraction, basis/ethanol/diesel proxies, and NASS targets (yield, harvested acres).  \n",
    "- Helper functions to **download, cache, and aggregate** to counties (area-weighted) and compute the engineered features (e.g., **hot-day counts**, **precip‚àíET**, **VPD proxy**, NDVI peak and anomaly, **USDM D2+** time-in-class).\n",
    "\n",
    "> ‚ö†Ô∏è **Requirements**: Internet access, and the following Python packages:  \n",
    "`pip install geopandas pandas numpy xarray rioxarray rasterio rasterstats requests tqdm pyproj shapely fiona py7zr`  \n",
    "Optional for speed: `pip install aiohttp`\n",
    "\n",
    "> üîê **Credentials you must provide** (set in the next cell):  \n",
    "- **NASA Earthdata** username/password (for GLDAS) ‚Äî used via `.netrc` cookie login.  \n",
    "- **USDA NASS QuickStats API key** (for county targets).  \n",
    "- **AppEEARS token** (optional; for MODIS NDVI); we also provide a Sentinel-2 fallback via STAC.\n",
    "\n",
    "**Output**\n",
    "- A single, tidy CSV: `/mnt/data/mn_minimal_schema.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d7c1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Helper: Robust xarray opener for gridMET (OPeNDAP or local .nc) ---\n",
    "import os, io, sys, requests\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "\n",
    "GRIDMET_BASE = \"https://www.northwestknowledge.net/thredds\"\n",
    "CACHE_NC_DIR = Path(\"data_cache/gridmet_nc\")\n",
    "CACHE_NC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def open_gridmet(var: str, year: int):\n",
    "    \"\"\"Open gridMET <var>_<year>.nc using OPeNDAP if possible; \n",
    "    otherwise download the NetCDF locally and open with netCDF4/h5netcdf.\n",
    "    \n",
    "    var in {'pr','tmmx','tmmn','vpd','eto',...}\n",
    "    \"\"\"\n",
    "    # Try OPeNDAP via pydap engine\n",
    "    # OPeNDAP URL (dodsC):\n",
    "    opendap_url = f\"{GRIDMET_BASE}/dodsC/MET/{var}_{year}.nc\"\n",
    "    try:\n",
    "        ds = xr.open_dataset(opendap_url, engine=\"pydap\", chunks={})\n",
    "        # Touch a small piece to validate\n",
    "        _ = list(ds.variables)[:1]\n",
    "        return ds\n",
    "    except Exception as e:\n",
    "        # Fallback: HTTPServer 'fileServer' direct download\n",
    "        http_url = f\"{GRIDMET_BASE}/fileServer/MET/{var}_{year}.nc\"\n",
    "        local_nc = CACHE_NC_DIR / f\"{var}_{year}.nc\"\n",
    "        if not local_nc.exists():\n",
    "            r = requests.get(http_url, stream=True, timeout=300)\n",
    "            r.raise_for_status()\n",
    "            with open(local_nc, \"wb\") as f:\n",
    "                for chunk in r.iter_content(1<<20):\n",
    "                    if chunk: f.write(chunk)\n",
    "        # Try netCDF4 first, then h5netcdf\n",
    "        try:\n",
    "            return xr.open_dataset(local_nc, engine=\"netCDF4\")\n",
    "        except Exception:\n",
    "            return xr.open_dataset(local_nc, engine=\"h5netcdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b59c5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- USER CONFIG ---\n",
    "START_YEAR = 2000\n",
    "END_YEAR   = 2024\n",
    "\n",
    "# Horizons to compute (use last day of month as cutoff)\n",
    "HORIZONS = {\n",
    "    \"FEB\": {\"month\": 2,  \"label\": \"FEB\"},\n",
    "    \"MAR\": {\"month\": 3,  \"label\": \"MAR\"},\n",
    "    \"APR\": {\"month\": 4,  \"label\": \"APR\"},\n",
    "    \"MAY\": {\"month\": 5,  \"label\": \"MAY\"},\n",
    "    \"JUN\": {\"month\": 6,  \"label\": \"JUN\"},\n",
    "    \"JUL\": {\"month\": 7,  \"label\": \"JUL\"},\n",
    "    \"AUG\": {\"month\": 8,  \"label\": \"AUG\"},\n",
    "}\n",
    "\n",
    "# Paths (cache)\n",
    "DATA_DIR = \"data_cache\"\n",
    "PRISM_DIR = f\"{DATA_DIR}/prism\"\n",
    "GLDAS_DIR = f\"{DATA_DIR}/gldas\"\n",
    "USDM_DIR  = f\"{DATA_DIR}/usdm\"\n",
    "CDL_DIR   = f\"{DATA_DIR}/cdl\"\n",
    "SOIL_DIR  = f\"{DATA_DIR}/soils\"\n",
    "NDVI_DIR  = f\"{DATA_DIR}/ndvi\"\n",
    "AUX_DIR   = f\"{DATA_DIR}/aux\"\n",
    "\n",
    "# üîê Credentials (fill in as needed)\n",
    "# NASA Earthdata (GLDAS). You should also create a ~/.netrc file for requests to use.\n",
    "EARTHDATA_USERNAME = \"rocketman01\"\n",
    "EARTHDATA_PASSWORD = \"Starship05!\"\n",
    "\n",
    "# NASS QuickStats\n",
    "NASS_API_KEY = \"606153B9-DDAD-30FB-88EE-5BCC60DEE678\"\n",
    "\n",
    "# AppEEARS (optional for MODIS NDVI). If not provided, the Sentinel-2 fallback is used.\n",
    "APPEEARS_TOKEN = None  # e.g., \"eyJhbGciOi...\"\n",
    "\n",
    "# Diesel / Ethanol / Basis: we include example endpoints & parsers. Some require free registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a07fed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tngzj\\AppData\\Local\\Temp\\ipykernel_38024\\1903551031.py:56: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  MN_BOUNDS = MN_COUNTIES.to_crs(4326).unary_union.bounds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>NAME</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27095</td>\n",
       "      <td>Mille Lacs</td>\n",
       "      <td>POLYGON ((-93.43244 46.06803, -93.43244 46.067...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27045</td>\n",
       "      <td>Fillmore</td>\n",
       "      <td>POLYGON ((-92.08948 43.50068, -92.08997 43.500...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27073</td>\n",
       "      <td>Lac qui Parle</td>\n",
       "      <td>POLYGON ((-96.22617 45.21949, -96.22608 45.219...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27085</td>\n",
       "      <td>McLeod</td>\n",
       "      <td>POLYGON ((-94.13242 44.71755, -94.15264 44.717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27153</td>\n",
       "      <td>Todd</td>\n",
       "      <td>POLYGON ((-94.64365 45.90727, -94.64365 45.906...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fips           NAME                                           geometry\n",
       "0  27095     Mille Lacs  POLYGON ((-93.43244 46.06803, -93.43244 46.067...\n",
       "1  27045       Fillmore  POLYGON ((-92.08948 43.50068, -92.08997 43.500...\n",
       "2  27073  Lac qui Parle  POLYGON ((-96.22617 45.21949, -96.22608 45.219...\n",
       "3  27085         McLeod  POLYGON ((-94.13242 44.71755, -94.15264 44.717...\n",
       "4  27153           Todd  POLYGON ((-94.64365 45.90727, -94.64365 45.906..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os, io, sys, json, math, zipfile, textwrap, datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box, Point, Polygon\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "\n",
    "import requests\n",
    "\n",
    "# Ensure cache dirs\n",
    "for d in [DATA_DIR, PRISM_DIR, GLDAS_DIR, USDM_DIR, CDL_DIR, SOIL_DIR, NDVI_DIR, AUX_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def month_range(start_year, end_year):\n",
    "    dates = []\n",
    "    for y in range(start_year, end_year+1):\n",
    "        for m in range(1, 13):\n",
    "            dates.append(dt.date(y, m, 1))\n",
    "    return dates\n",
    "\n",
    "def end_of_month(d: dt.date) -> dt.date:\n",
    "    if d.month == 12:\n",
    "        return dt.date(d.year, 12, 31)\n",
    "    next_m = dt.date(d.year, d.month+1, 1)\n",
    "    return next_m - dt.timedelta(days=1)\n",
    "\n",
    "def ensure_state_counties_mn() -> gpd.GeoDataFrame:\n",
    "    \"\"\"Download and return Minnesota county polygons from TIGER/Line (Census).\"\"\"\n",
    "    url = \"https://www2.census.gov/geo/tiger/TIGER2023/COUNTY/tl_2023_us_county.zip\"\n",
    "    zpath = os.path.join(AUX_DIR, \"tl_2023_us_county.zip\")\n",
    "    if not os.path.exists(zpath):\n",
    "        r = requests.get(url, timeout=120)\n",
    "        r.raise_for_status()\n",
    "        with open(zpath, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "    with zipfile.ZipFile(zpath) as z:\n",
    "        z.extractall(os.path.join(AUX_DIR, \"tl_2023_us_county\"))\n",
    "    g = gpd.read_file(os.path.join(AUX_DIR, \"tl_2023_us_county\", \"tl_2023_us_county.shp\"))\n",
    "    # Minnesota FIPS = 27\n",
    "    g = g[g[\"STATEFP\"] == \"27\"].to_crs(4326)\n",
    "    g[\"fips\"] = g[\"STATEFP\"] + g[\"COUNTYFP\"]\n",
    "    g = g[[\"fips\", \"NAME\", \"geometry\"]].reset_index(drop=True)\n",
    "    return g\n",
    "\n",
    "MN_COUNTIES = ensure_state_counties_mn()\n",
    "MN_BOUNDS = MN_COUNTIES.to_crs(4326).unary_union.bounds\n",
    "MN_COUNTIES.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433f328",
   "metadata": {},
   "source": [
    "\n",
    "## PRISM (Temperature & Precipitation)\n",
    "\n",
    "We pull **monthly** PRISM tmax, tmin, and precipitation grids and compute:\n",
    "- `prism_prcp_mm`, `prism_tmax_c`, `prism_tmin_c`\n",
    "- **Hot-day counts** (>32¬∞C or >35¬∞C approximations via monthly degree-day proxy)\n",
    "- **VPD proxy** using monthly tmin/tmax\n",
    "\n",
    "> Note: For exact daily hot-day counts and VPD, prefer daily PRISM. Monthly approximations are used here for efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae5f51ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tngzj\\AppData\\Local\\Temp\\ipykernel_38024\\2815307866.py:60: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt1 = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\tngzj\\AppData\\Local\\Temp\\ipykernel_38024\\2815307866.py:131: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  pts = pts[pts.geometry.within(MN_COUNTIES.unary_union)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PRISM CSV ‚Üí prism_df shape: (15312, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>prism_prcp_mm</th>\n",
       "      <th>prism_tmax_c</th>\n",
       "      <th>prism_tmin_c</th>\n",
       "      <th>vpd_proxy</th>\n",
       "      <th>heat_days_35c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27001</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>18.542</td>\n",
       "      <td>-7.222222</td>\n",
       "      <td>-17.611111</td>\n",
       "      <td>0.0635</td>\n",
       "      <td>2.035552e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27001</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>5.334</td>\n",
       "      <td>-2.611111</td>\n",
       "      <td>-17.000000</td>\n",
       "      <td>0.1535</td>\n",
       "      <td>2.041607e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27001</td>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "      <td>24.384</td>\n",
       "      <td>8.388889</td>\n",
       "      <td>-3.833333</td>\n",
       "      <td>0.3180</td>\n",
       "      <td>4.995640e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27001</td>\n",
       "      <td>2010</td>\n",
       "      <td>4</td>\n",
       "      <td>23.622</td>\n",
       "      <td>16.333333</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.6725</td>\n",
       "      <td>2.652575e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27001</td>\n",
       "      <td>2010</td>\n",
       "      <td>5</td>\n",
       "      <td>78.232</td>\n",
       "      <td>19.388889</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>0.7730</td>\n",
       "      <td>1.221897e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fips  year  month  prism_prcp_mm  prism_tmax_c  prism_tmin_c  vpd_proxy  \\\n",
       "0  27001  2010      1         18.542     -7.222222    -17.611111     0.0635   \n",
       "1  27001  2010      2          5.334     -2.611111    -17.000000     0.1535   \n",
       "2  27001  2010      3         24.384      8.388889     -3.833333     0.3180   \n",
       "3  27001  2010      4         23.622     16.333333      0.888889     0.6725   \n",
       "4  27001  2010      5         78.232     19.388889      5.555556     0.7730   \n",
       "\n",
       "   heat_days_35c  \n",
       "0   2.035552e-08  \n",
       "1   2.041607e-07  \n",
       "2   4.995640e-05  \n",
       "3   2.652575e-03  \n",
       "4   1.221897e-02  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "PRISM_CSV_PATH = \"data/PRISM_ppt_tmin_tmean_tmax_tdmean_vpdmin_vpdmax_stable_4km_201001_202408.csv\"\n",
    "\n",
    "def load_prism_csv(path):\n",
    "    # Read without type coercion to control conversions\n",
    "    df = pd.read_csv(path)\n",
    "    orig_cols = list(df.columns)\n",
    "\n",
    "    # Detect units from original column names\n",
    "    def has_unit(col_substr, unit_substr):\n",
    "        return any((col_substr.lower() in oc.lower()) and (unit_substr.lower() in oc.lower()) for oc in orig_cols)\n",
    "\n",
    "    ppt_in_inches = has_unit(\"ppt\", \"inch\")\n",
    "    temp_in_f     = has_unit(\"tmin\", \"deg\") or has_unit(\"tmax\", \"deg\") or has_unit(\"tmean\",\"deg\")\n",
    "    vpd_in_hpa    = has_unit(\"vpd\", \"hpa\")\n",
    "\n",
    "    # Standardize column names: lower, remove parentheses content, replace spaces, remove unit tokens\n",
    "    def std_name(c):\n",
    "        cl = c.strip().lower()\n",
    "        import re as _re\n",
    "        cl = _re.sub(r\"\\(.*?\\)\", \"\", cl)  # drop parenthetical units\n",
    "        cl = cl.replace(\"degrees\", \"\").replace(\"degree\", \"\").replace(\"deg\", \"\")\n",
    "        cl = cl.replace(\"f\", \"\").replace(\"c\", \"\")\n",
    "        cl = cl.replace(\"inches\", \"\").replace(\"inch\", \"\")\n",
    "        cl = cl.replace(\"hpa\",\"\").replace(\"kpa\",\"\")\n",
    "        while \"  \" in cl:\n",
    "            cl = cl.replace(\"  \", \" \")\n",
    "        cl = cl.strip().replace(\" \", \"_\")\n",
    "        cl = cl.replace(\"__\",\"_\")\n",
    "        return cl\n",
    "\n",
    "    df.columns = [std_name(c) for c in df.columns]\n",
    "\n",
    "    # Candidate date columns\n",
    "    date_col = None\n",
    "    for cand in [\"date\",\"time\",\"day\",\"month_year\",\"yyyymm\"]:\n",
    "        if cand in df.columns:\n",
    "            date_col = cand; break\n",
    "    if date_col is None:\n",
    "        # Try 'year' and 'month' pair\n",
    "        if \"year\" in df.columns and \"month\" in df.columns:\n",
    "            df[\"date\"] = pd.to_datetime(df[\"year\"].astype(int).astype(str) + \"-\" + df[\"month\"].astype(int).astype(str) + \"-01\")\n",
    "            date_col = \"date\"\n",
    "        else:\n",
    "            # Try to infer from a column with mm/dd/yyyy strings\n",
    "            for c in df.columns:\n",
    "                if df[c].astype(str).str.match(r\"^\\d{1,2}/\\d{1,2}/\\d{4}$\").any():\n",
    "                    date_col = c\n",
    "                    break\n",
    "    if date_col is None:\n",
    "        raise ValueError(\"Could not detect a date column in the PRISM CSV.\")\n",
    "\n",
    "    # Parse dates robustly\n",
    "    def parse_date_series(s):\n",
    "        s = s.astype(str).str.strip()\n",
    "        dt1 = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "        if dt1.isna().mean() > 0.2:\n",
    "            try_dt = pd.to_datetime(s, format=\"%m/%d/%Y\", errors=\"coerce\")\n",
    "            dt1 = try_dt.where(~try_dt.isna(), dt1)\n",
    "        return dt1\n",
    "\n",
    "    df[\"date\"] = parse_date_series(df[date_col])\n",
    "    if df[\"date\"].isna().all():\n",
    "        raise ValueError(\"Failed to parse dates in PRISM CSV‚Äîplease check the date column format.\")\n",
    "    df[\"year\"] = df[\"date\"].dt.year\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "\n",
    "    # Map likely variable names after std_name\n",
    "    lon_col = \"longitude\" if \"longitude\" in df.columns else (\"lon\" if \"lon\" in df.columns else None)\n",
    "    lat_col = \"latitude\"  if \"latitude\"  in df.columns else (\"lat\" if \"lat\" in df.columns else None)\n",
    "\n",
    "    rename = {}\n",
    "    for cand in [\"ppt\",\"precip\",\"precipitation\"]:\n",
    "        if cand in df.columns: rename[cand] = \"ppt_raw\"\n",
    "    for cand in [\"tmin\",\"min_temp\",\"tmin_degrees\",\"tmin_degrees_f\"]:\n",
    "        if cand in df.columns: rename[cand] = \"tmin_raw\"\n",
    "    for cand in [\"tmax\",\"max_temp\",\"tmax_degrees\",\"tmax_degrees_f\"]:\n",
    "        if cand in df.columns: rename[cand] = \"tmax_raw\"\n",
    "    for cand in [\"tmean\",\"tavg\",\"mean_temp\"]:\n",
    "        if cand in df.columns: rename[cand] = \"tmean_raw\"\n",
    "    for cand in [\"tdmean\",\"dewpoint_mean\",\"dewpoint\"]:\n",
    "        if cand in df.columns: rename[cand] = \"tdmean_raw\"\n",
    "    for cand in [\"vpdmin\",\"vpd_min\"]:\n",
    "        if cand in df.columns: rename[cand] = \"vpdmin_raw\"\n",
    "    for cand in [\"vpdmax\",\"vpd_max\"]:\n",
    "        if cand in df.columns: rename[cand] = \"vpdmax_raw\"\n",
    "\n",
    "    df = df.rename(columns=rename)\n",
    "\n",
    "    # Unit conversions\n",
    "    if \"ppt_raw\" in df.columns:\n",
    "        ppt = pd.to_numeric(df[\"ppt_raw\"], errors=\"coerce\")\n",
    "        if ppt_in_inches:\n",
    "            ppt = ppt * 25.4\n",
    "        df[\"prism_prcp_mm\"] = ppt\n",
    "\n",
    "    def f_to_c(s):\n",
    "        s = pd.to_numeric(s, errors=\"coerce\")\n",
    "        return (s - 32.0) * 5.0/9.0\n",
    "    if \"tmin_raw\" in df.columns:\n",
    "        df[\"prism_tmin_c\"] = f_to_c(df[\"tmin_raw\"]) if temp_in_f else pd.to_numeric(df[\"tmin_raw\"], errors=\"coerce\")\n",
    "    if \"tmax_raw\" in df.columns:\n",
    "        df[\"prism_tmax_c\"] = f_to_c(df[\"tmax_raw\"]) if temp_in_f else pd.to_numeric(df[\"tmax_raw\"], errors=\"coerce\")\n",
    "    if \"tmean_raw\" in df.columns:\n",
    "        df[\"tmean_c\"] = f_to_c(df[\"tmean_raw\"]) if temp_in_f else pd.to_numeric(df[\"tmean_raw\"], errors=\"coerce\")\n",
    "    if \"tdmean_raw\" in df.columns:\n",
    "        df[\"tdmean_c\"] = f_to_c(df[\"tdmean_raw\"]) if temp_in_f else pd.to_numeric(df[\"tdmean_raw\"], errors=\"coerce\")\n",
    "\n",
    "    vpdmin = pd.to_numeric(df[\"vpdmin_raw\"], errors=\"coerce\") if \"vpdmin_raw\" in df.columns else np.nan\n",
    "    vpdmax = pd.to_numeric(df[\"vpdmax_raw\"], errors=\"coerce\") if \"vpdmax_raw\" in df.columns else np.nan\n",
    "    if isinstance(vpdmin, pd.Series) or isinstance(vpdmax, pd.Series):\n",
    "        if vpd_in_hpa:\n",
    "            if isinstance(vpdmin, pd.Series): vpdmin = vpdmin * 0.1\n",
    "            if isinstance(vpdmax, pd.Series): vpdmax = vpdmax * 0.1\n",
    "        df[\"vpd_proxy\"] = pd.concat([vpdmin, vpdmax], axis=1).mean(axis=1)\n",
    "\n",
    "    if \"prism_tmax_c\" in df.columns:\n",
    "        df[\"heat_days_35c\"] = 30.0 * (1/(1+np.exp(-(df[\"prism_tmax_c\"] - 35.0)/2)))\n",
    "\n",
    "    if (lon_col is not None) and (lat_col is not None):\n",
    "        pts = gpd.GeoDataFrame(\n",
    "            df[[\"year\",\"month\",\"prism_prcp_mm\",\"prism_tmax_c\",\"prism_tmin_c\",\"vpd_proxy\",\"heat_days_35c\", lon_col, lat_col]].copy(),\n",
    "            geometry=gpd.points_from_xy(pd.to_numeric(df[lon_col], errors=\"coerce\"),\n",
    "                                        pd.to_numeric(df[lat_col], errors=\"coerce\")),\n",
    "            crs=4326\n",
    "        ).dropna(subset=[\"geometry\"])\n",
    "        pts = pts[pts.geometry.within(MN_COUNTIES.unary_union)]\n",
    "        joined = gpd.sjoin(pts, MN_COUNTIES[[\"fips\",\"geometry\"]], how=\"inner\", predicate=\"within\")\n",
    "        prism_df_from_csv = joined.groupby([\"fips\",\"year\",\"month\"], as_index=False).agg({\n",
    "            \"prism_prcp_mm\":\"mean\",\n",
    "            \"prism_tmax_c\":\"mean\",\n",
    "            \"prism_tmin_c\":\"mean\",\n",
    "            \"vpd_proxy\":\"mean\",\n",
    "            \"heat_days_35c\":\"mean\"\n",
    "        })\n",
    "    elif \"fips\" in df.columns:\n",
    "        temp = df.rename(columns={\"fips\":\"fips_raw\"})\n",
    "        temp[\"fips\"] = temp[\"fips_raw\"].astype(str).str.zfill(5)\n",
    "        prism_df_from_csv = temp.groupby([\"fips\",\"year\",\"month\"], as_index=False).agg({\n",
    "            \"prism_prcp_mm\":\"mean\",\n",
    "            \"prism_tmax_c\":\"mean\",\n",
    "            \"prism_tmin_c\":\"mean\",\n",
    "            \"vpd_proxy\":\"mean\",\n",
    "            \"heat_days_35c\":\"mean\"\n",
    "        })\n",
    "    else:\n",
    "        base = df.groupby([\"year\",\"month\"], as_index=False).agg({\n",
    "            \"prism_prcp_mm\":\"mean\",\n",
    "            \"prism_tmax_c\":\"mean\",\n",
    "            \"prism_tmin_c\":\"mean\",\n",
    "            \"vpd_proxy\":\"mean\",\n",
    "            \"heat_days_35c\":\"mean\"\n",
    "        })\n",
    "        cnt = MN_COUNTIES[[\"fips\"]].copy()\n",
    "        cnt[\"key\"]=1; base[\"key\"]=1\n",
    "        prism_df_from_csv = cnt.merge(base, on=\"key\").drop(columns=[\"key\"])\n",
    "\n",
    "    need_cols = [\"fips\",\"year\",\"month\",\"prism_prcp_mm\",\"prism_tmax_c\",\"prism_tmin_c\",\"vpd_proxy\",\"heat_days_35c\"]\n",
    "    for c in need_cols:\n",
    "        if c not in prism_df_from_csv.columns:\n",
    "            prism_df_from_csv[c] = np.nan\n",
    "    prism_df_from_csv = prism_df_from_csv[need_cols].sort_values([\"fips\",\"year\",\"month\"]).reset_index(drop=True)\n",
    "    return prism_df_from_csv\n",
    "\n",
    "# Build from uploaded CSV and set as prism_df\n",
    "prism_df = load_prism_csv(PRISM_CSV_PATH)\n",
    "print(\"Loaded PRISM CSV ‚Üí prism_df shape:\", prism_df.shape)\n",
    "display(prism_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a06b397-372a-46a7-8715-626393cefd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_df.to_csv('prism_.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ec34c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: manual_inputs/prism/mn_prism_bulk_points.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tngzj\\AppData\\Local\\Temp\\ipykernel_38024\\1829695213.py:10: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  mn[\"centroid\"] = mn.geometry.centroid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45.938046</td>\n",
       "      <td>-93.630095</td>\n",
       "      <td>MilleLacs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43.674005</td>\n",
       "      <td>-92.090176</td>\n",
       "      <td>Fillmore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.995487</td>\n",
       "      <td>-96.173478</td>\n",
       "      <td>LacquiParle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44.823559</td>\n",
       "      <td>-94.272436</td>\n",
       "      <td>McLeod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.070627</td>\n",
       "      <td>-94.897603</td>\n",
       "      <td>Todd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    latitude  longitude         name\n",
       "0  45.938046 -93.630095    MilleLacs\n",
       "1  43.674005 -92.090176     Fillmore\n",
       "2  44.995487 -96.173478  LacquiParle\n",
       "3  44.823559 -94.272436       McLeod\n",
       "4  46.070627 -94.897603         Todd"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Export Minnesota county centroids for PRISM Bulk (‚â§12-char names)\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "OUT_DIR = \"manual_inputs/prism\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# MN_COUNTIES is already built earlier in the notebook\n",
    "mn = MN_COUNTIES.copy()\n",
    "mn[\"centroid\"] = mn.geometry.centroid\n",
    "def trim12(s): \n",
    "    return (s.replace(\" \", \"\")[:12]) if isinstance(s,str) else \"\"\n",
    "\n",
    "points = pd.DataFrame({\n",
    "    \"latitude\": mn[\"centroid\"].y.round(6),\n",
    "    \"longitude\": mn[\"centroid\"].x.round(6),\n",
    "    \"name\": mn[\"NAME\"].apply(trim12)  # optional, 12 chars max\n",
    "})\n",
    "centroids_csv = f\"{OUT_DIR}/mn_prism_bulk_points.csv\"\n",
    "points.to_csv(centroids_csv, index=False)\n",
    "print(\"Wrote:\", centroids_csv)\n",
    "points.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd478e02",
   "metadata": {},
   "source": [
    "\n",
    "## GLDAS (Noah 0.25¬∞, monthly)\n",
    "\n",
    "We fetch monthly GLDAS variables and aggregate to counties:\n",
    "- `gldas_soil_moisture_0_10cm`, `gldas_et_mm`, `gldas_rn_wm2`\n",
    "\n",
    "> ‚ö†Ô∏è Requires **NASA Earthdata** login via `.netrc` or session cookies. See: https://disc.gsfc.nasa.gov/earthdata-login\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c254ca",
   "metadata": {},
   "source": [
    "\n",
    "## U.S. Drought Monitor (USDM)\n",
    "\n",
    "We compute:\n",
    "- `usdm_pct_D2plus`: percent of county area in D2‚ÄìD4 during the month\n",
    "- `usdm_days_D2plus`: number of **days** in D2+ within the month\n",
    "\n",
    "We download weekly shapefiles and aggregate by county and by day-count per month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e82b213d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USDM years: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [05:43<00:00, 13.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USDM built: (26100, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>usdm_pct_D2plus</th>\n",
       "      <th>usdm_days_D2plus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27095</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27045</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27073</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27085</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27153</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fips  year  month  usdm_pct_D2plus  usdm_days_D2plus\n",
       "0  27095  2000      1              0.0                 0\n",
       "1  27045  2000      1              0.0                 0\n",
       "2  27073  2000      1              0.0                 0\n",
       "3  27085  2000      1              0.0                 0\n",
       "4  27153  2000      1              0.0                 0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Robust USDM builder (fills zeros if downloads are missing) ---\n",
    "import datetime as dt, os, zipfile, requests, pandas as pd, geopandas as gpd, numpy as np\n",
    "\n",
    "USDM_DIR  = USDM_DIR if \"USDM_DIR\" in globals() else \"data_cache/usdm\"\n",
    "os.makedirs(USDM_DIR, exist_ok=True)\n",
    "\n",
    "def _usdm_zip_candidates(d: dt.date):\n",
    "    # weekly product usually Thursday; filenames use YYYYMMDD\n",
    "    dstr = d.strftime(\"%Y%m%d\")\n",
    "    # modern and legacy paths\n",
    "    return [\n",
    "        f\"https://droughtmonitor.unl.edu/data/shapefiles_m/USDM_{dstr}_M.zip\",\n",
    "        f\"https://droughtmonitor.unl.edu/data/shapefiles/USDM_{dstr}_M.zip\",\n",
    "    ]\n",
    "\n",
    "def _nearest_thursdays_in_month(year: int, month: int):\n",
    "    first = dt.date(year, month, 1)\n",
    "    last  = (first.replace(day=28) + dt.timedelta(days=4)).replace(day=1) - dt.timedelta(days=1)\n",
    "    d = first\n",
    "    thurs = []\n",
    "    while d <= last:\n",
    "        # Thursday = 3 (Mon=0)\n",
    "        if d.weekday() == 3:\n",
    "            thurs.append(d)\n",
    "        d += dt.timedelta(days=1)\n",
    "    # if none (rare), include last day as fallback\n",
    "    return thurs or [last]\n",
    "\n",
    "def _fetch_usdm_weekly_zip(d: dt.date):\n",
    "    for url in _usdm_zip_candidates(d):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=90)\n",
    "            if r.status_code == 200:\n",
    "                zpath = os.path.join(USDM_DIR, os.path.basename(url))\n",
    "                with open(zpath, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                # extract once\n",
    "                with zipfile.ZipFile(zpath) as z:\n",
    "                    z.extractall(os.path.join(USDM_DIR, os.path.splitext(os.path.basename(zpath))[0]))\n",
    "                return zpath\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def build_usdm_table(start_year=START_YEAR, end_year=END_YEAR):\n",
    "    rows = []\n",
    "    for y in tqdm(range(start_year, end_year+1), desc=\"USDM years\"):\n",
    "        for m in range(1, 13):\n",
    "            thursdays = _nearest_thursdays_in_month(y, m)\n",
    "            weekly_polys = []\n",
    "            for d in thursdays:\n",
    "                z = _fetch_usdm_weekly_zip(d)\n",
    "                if z:\n",
    "                    folder = os.path.join(USDM_DIR, os.path.splitext(os.path.basename(z))[0])\n",
    "                    shp = None\n",
    "                    for f in os.listdir(folder):\n",
    "                        if f.lower().endswith(\".shp\"):\n",
    "                            shp = os.path.join(folder, f)\n",
    "                            break\n",
    "                    if shp:\n",
    "                        g = gpd.read_file(shp).to_crs(4326)\n",
    "                        # Level column sometimes named DM (D0..D4)\n",
    "                        lvl = g.get(\"DM\", None)\n",
    "                        if lvl is None:\n",
    "                            continue\n",
    "                        g[\"level\"] = g[\"DM\"].map({\"D0\":0,\"D1\":1,\"D2\":2,\"D3\":3,\"D4\":4}).fillna(-1)\n",
    "                        weekly_polys.append(g[[\"level\",\"geometry\"]])\n",
    "\n",
    "            # If we still found nothing for this month ‚Üí fill zeros so concat never fails\n",
    "            if not weekly_polys:\n",
    "                for _, crow in MN_COUNTIES.iterrows():\n",
    "                    rows.append(pd.DataFrame([{\n",
    "                        \"fips\": crow.fips, \"year\": y, \"month\": m,\n",
    "                        \"usdm_pct_D2plus\": 0.0, \"usdm_days_D2plus\": 0\n",
    "                    }]))\n",
    "                continue\n",
    "\n",
    "            # Compute area share in D2+; assume weekly maps cover ~7 days\n",
    "            month_res = []\n",
    "            for _, crow in MN_COUNTIES.iterrows():\n",
    "                cgeom = gpd.GeoSeries([crow.geometry], crs=4326)\n",
    "                total_area = cgeom.to_crs(3857).area.values[0]\n",
    "                days_D2p = 0\n",
    "                area_pcts = []\n",
    "                for wg in weekly_polys:\n",
    "                    inter = gpd.overlay(gpd.GeoDataFrame(geometry=cgeom), wg, how=\"intersection\")\n",
    "                    if inter.empty:\n",
    "                        area_pcts.append(0.0); continue\n",
    "                    inter = inter[inter[\"level\"] >= 2]\n",
    "                    if inter.empty:\n",
    "                        area_pcts.append(0.0); continue\n",
    "                    inter_area = inter.to_crs(3857).area.sum()\n",
    "                    pct = float(inter_area / total_area * 100.0)\n",
    "                    area_pcts.append(pct)\n",
    "                    if pct > 0:\n",
    "                        days_D2p += 7\n",
    "                usdm_pct = float(np.mean(area_pcts)) if area_pcts else 0.0\n",
    "                month_res.append({\n",
    "                    \"fips\": crow.fips, \"year\": y, \"month\": m,\n",
    "                    \"usdm_pct_D2plus\": usdm_pct,\n",
    "                    \"usdm_days_D2plus\": int(days_D2p)\n",
    "                })\n",
    "            rows.append(pd.DataFrame(month_res))\n",
    "\n",
    "    # Even if everything failed, guarantee a zeroed frame (so downstream doesn‚Äôt break)\n",
    "    if not rows:\n",
    "        zero = []\n",
    "        for y in range(start_year, end_year+1):\n",
    "            for m in range(1,13):\n",
    "                for _, crow in MN_COUNTIES.iterrows():\n",
    "                    zero.append({\"fips\": crow.fips, \"year\": y, \"month\": m,\n",
    "                                 \"usdm_pct_D2plus\": 0.0, \"usdm_days_D2plus\": 0})\n",
    "        return pd.DataFrame(zero)\n",
    "\n",
    "    return pd.concat(rows, ignore_index=True)\n",
    "\n",
    "usdm_df = build_usdm_table()\n",
    "print(\"USDM built:\", usdm_df.shape)\n",
    "usdm_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b71953",
   "metadata": {},
   "source": [
    "\n",
    "## ENSO ‚Äî Oceanic Ni√±o Index (ONI)\n",
    "\n",
    "We ingest NOAA's ONI table and join monthly with a 1‚Äì3 month lead option.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35a9af22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USDM years: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [05:41<00:00, 13.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USDM built: (26100, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>usdm_pct_D2plus</th>\n",
       "      <th>usdm_days_D2plus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27095</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27045</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27073</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27085</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27153</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fips  year  month  usdm_pct_D2plus  usdm_days_D2plus\n",
       "0  27095  2000      1              0.0                 0\n",
       "1  27045  2000      1              0.0                 0\n",
       "2  27073  2000      1              0.0                 0\n",
       "3  27085  2000      1              0.0                 0\n",
       "4  27153  2000      1              0.0                 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Robust USDM builder (fills zeros if downloads are missing), CRS-safe & geometry-safe ---\n",
    "import datetime as dt, os, zipfile, requests, pandas as pd, geopandas as gpd, numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "USDM_DIR  = USDM_DIR if \"USDM_DIR\" in globals() else \"data_cache/usdm\"\n",
    "os.makedirs(USDM_DIR, exist_ok=True)\n",
    "\n",
    "# Make sure MN_COUNTIES exists and is EPSG:4326\n",
    "assert \"MN_COUNTIES\" in globals(), \"MN_COUNTIES GeoDataFrame must be defined earlier.\"\n",
    "if MN_COUNTIES.crs is None or str(MN_COUNTIES.crs).lower() not in (\"epsg:4326\",\"4326\"):\n",
    "    MN_COUNTIES = MN_COUNTIES.to_crs(4326)\n",
    "\n",
    "def _usdm_zip_candidates(d: dt.date):\n",
    "    dstr = d.strftime(\"%Y%m%d\")\n",
    "    return [\n",
    "        f\"https://droughtmonitor.unl.edu/data/shapefiles_m/USDM_{dstr}_M.zip\",\n",
    "        f\"https://droughtmonitor.unl.edu/data/shapefiles/USDM_{dstr}_M.zip\",\n",
    "    ]\n",
    "\n",
    "def _nearest_thursdays_in_month(year: int, month: int):\n",
    "    first = dt.date(year, month, 1)\n",
    "    last  = (first.replace(day=28) + dt.timedelta(days=4)).replace(day=1) - dt.timedelta(days=1)\n",
    "    d = first\n",
    "    thurs = []\n",
    "    while d <= last:\n",
    "        if d.weekday() == 3:  # Thursday\n",
    "            thurs.append(d)\n",
    "        d += dt.timedelta(days=1)\n",
    "    return thurs or [last]\n",
    "\n",
    "def _fetch_usdm_weekly_zip(d: dt.date):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; data-pipeline; +https://example.org)\"}\n",
    "    for url in _usdm_zip_candidates(d):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=120, headers=headers)\n",
    "            if r.status_code == 200 and r.content:\n",
    "                zpath = os.path.join(USDM_DIR, os.path.basename(url))\n",
    "                with open(zpath, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                with zipfile.ZipFile(zpath) as z:\n",
    "                    z.extractall(os.path.join(USDM_DIR, os.path.splitext(os.path.basename(zpath))[0]))\n",
    "                return zpath\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def _read_usdm_shp_from_zip(zpath: str) -> gpd.GeoDataFrame | None:\n",
    "    folder = os.path.join(USDM_DIR, os.path.splitext(os.path.basename(zpath))[0])\n",
    "    shp = None\n",
    "    for f in os.listdir(folder):\n",
    "        if f.lower().endswith(\".shp\"):\n",
    "            shp = os.path.join(folder, f); break\n",
    "    if not shp: return None\n",
    "    g = gpd.read_file(shp)\n",
    "    if g.crs is None:\n",
    "        g = g.set_crs(4326, allow_override=True)\n",
    "    else:\n",
    "        g = g.to_crs(4326)\n",
    "    # Normalize level column; some files use 'DM'\n",
    "    if \"DM\" not in g.columns:\n",
    "        return None\n",
    "    g[\"level\"] = g[\"DM\"].map({\"D0\":0,\"D1\":1,\"D2\":2,\"D3\":3,\"D4\":4}).fillna(-1).astype(int)\n",
    "    # Fix invalid geometries that break overlay\n",
    "    g[\"geometry\"] = g.geometry.buffer(0)\n",
    "    # Keep only D2+ polygons; small speedup\n",
    "    g = g[g[\"level\"] >= 2][[\"level\",\"geometry\"]]\n",
    "    if g.empty:\n",
    "        return g\n",
    "    # Clip to MN bounding box for speed\n",
    "    g = g.clip(MN_COUNTIES.unary_union)\n",
    "    return g\n",
    "\n",
    "def build_usdm_table(start_year=START_YEAR, end_year=END_YEAR):\n",
    "    rows = []\n",
    "    # Precompute county areas in an equal-area projection\n",
    "    counties_aea = MN_COUNTIES.to_crs(5070)  # NAD83 / Conus Albers\n",
    "    county_area = counties_aea.area.values\n",
    "    county_ids  = MN_COUNTIES[\"fips\"].values\n",
    "\n",
    "    for y in tqdm(range(start_year, end_year+1), desc=\"USDM years\"):\n",
    "        for m in range(1, 13):\n",
    "            weekly_polys = []\n",
    "            for d in _nearest_thursdays_in_month(y, m):\n",
    "                z = _fetch_usdm_weekly_zip(d)\n",
    "                if not z: continue\n",
    "                g = _read_usdm_shp_from_zip(z)\n",
    "                if g is not None and not g.empty:\n",
    "                    weekly_polys.append(g)\n",
    "\n",
    "            # If nothing found ‚Üí fill zeros, keep pipeline moving\n",
    "            if not weekly_polys:\n",
    "                rows.append(pd.DataFrame({\n",
    "                    \"fips\": county_ids,\n",
    "                    \"year\": y,\n",
    "                    \"month\": m,\n",
    "                    \"usdm_pct_D2plus\": np.zeros(len(county_ids), dtype=float),\n",
    "                    \"usdm_days_D2plus\": np.zeros(len(county_ids), dtype=int)\n",
    "                }))\n",
    "                continue\n",
    "\n",
    "            # Compute area share per week vectorized via overlay with all counties at once\n",
    "            month_pct_list = []\n",
    "            days_D2p = np.zeros(len(county_ids), dtype=int)\n",
    "            for wg in weekly_polys:\n",
    "                try:\n",
    "                    inter = gpd.overlay(MN_COUNTIES[[\"fips\",\"geometry\"]], wg, how=\"intersection\")\n",
    "                except Exception:\n",
    "                    # Fallback: repair and retry\n",
    "                    MN_fixed = MN_COUNTIES.copy()\n",
    "                    MN_fixed[\"geometry\"] = MN_fixed.geometry.buffer(0)\n",
    "                    wg_fixed = wg.copy()\n",
    "                    wg_fixed[\"geometry\"] = wg_fixed.geometry.buffer(0)\n",
    "                    inter = gpd.overlay(MN_fixed[[\"fips\",\"geometry\"]], wg_fixed, how=\"intersection\")\n",
    "\n",
    "                if inter.empty:\n",
    "                    month_pct_list.append(np.zeros(len(county_ids), dtype=float))\n",
    "                    continue\n",
    "\n",
    "                inter_area = inter.to_crs(5070).area.groupby(inter[\"fips\"]).sum()\n",
    "                # Map to county order\n",
    "                pct = np.zeros(len(county_ids), dtype=float)\n",
    "                for i, f in enumerate(county_ids):\n",
    "                    if f in inter_area.index:\n",
    "                        pct[i] = float(inter_area.loc[f] / county_area[i] * 100.0)\n",
    "                month_pct_list.append(pct)\n",
    "                days_D2p += (pct > 0).astype(int) * 7  # ~7 days per map\n",
    "\n",
    "            # Average weekly percentages ‚Üí monthly percent area\n",
    "            if month_pct_list:\n",
    "                pct_month = np.vstack(month_pct_list).mean(axis=0)\n",
    "            else:\n",
    "                pct_month = np.zeros(len(county_ids), dtype=float)\n",
    "\n",
    "            rows.append(pd.DataFrame({\n",
    "                \"fips\": county_ids,\n",
    "                \"year\": y,\n",
    "                \"month\": m,\n",
    "                \"usdm_pct_D2plus\": pct_month,\n",
    "                \"usdm_days_D2plus\": days_D2p\n",
    "            }))\n",
    "\n",
    "    return pd.concat(rows, ignore_index=True)\n",
    "\n",
    "usdm_df = build_usdm_table()\n",
    "print(\"USDM built:\", usdm_df.shape)\n",
    "display(usdm_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f511284e",
   "metadata": {},
   "source": [
    "\n",
    "## Cropland Data Layer (CDL) ‚Äî Corn Fraction\n",
    "\n",
    "We compute the fraction of each county planted to **corn** using CDL (2008‚Äìpresent).  \n",
    "This uses the public AWS endpoint maintained by USDA/NASS (GeoTIFF per year/state or CONUS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49d3770c-a8b3-4bdb-b28a-312899889442",
   "metadata": {},
   "outputs": [],
   "source": [
    "usdm_df.to_csv('OceanicNinoIndex.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40dad996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CDL years:   0%|                                                                                | 0/17 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://www.nass.usda.gov/Research_and_Science/Cropland/Release/datasets/USDA_CDL_2008.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m         rows\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(rows, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 51\u001b[0m cdl_df \u001b[38;5;241m=\u001b[39m build_cdl_table()\n\u001b[0;32m     52\u001b[0m cdl_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[1;32mIn[41], line 45\u001b[0m, in \u001b[0;36mbuild_cdl_table\u001b[1;34m(start_year, end_year)\u001b[0m\n\u001b[0;32m     43\u001b[0m rows \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(start_year, \u001b[38;5;28mmin\u001b[39m(end_year, dt\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;241m.\u001b[39myear)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCDL years\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 45\u001b[0m     tif \u001b[38;5;241m=\u001b[39m fetch_cdl_year(y)\n\u001b[0;32m     46\u001b[0m     df \u001b[38;5;241m=\u001b[39m county_corn_fraction_from_cdl(tif, MN_COUNTIES)\n\u001b[0;32m     47\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39my; df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcdl_corn_fraction\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorn_frac\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[41], line 12\u001b[0m, in \u001b[0;36mfetch_cdl_year\u001b[1;34m(year)\u001b[0m\n\u001b[0;32m     10\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.nass.usda.gov/Research_and_Science/Cropland/Release/datasets/USDA_CDL_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m     r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     14\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(r\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1021\u001b[0m     )\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://www.nass.usda.gov/Research_and_Science/Cropland/Release/datasets/USDA_CDL_2008.tif"
     ]
    }
   ],
   "source": [
    "\n",
    "def fetch_cdl_year(year: int) -> Path:\n",
    "    # Use CONUS GeoTIFF (might be large). Alternatively, pull per-state tiles if available.\n",
    "    url = f\"https://www.nass.usda.gov/Research_and_Science/Cropland/Release/datasets/CDL_{year}_v6.tif\"\n",
    "    out = Path(CDL_DIR)/f\"CDL_{year}.tif\"\n",
    "    if out.exists():\n",
    "        return out\n",
    "    r = requests.get(url, timeout=300)\n",
    "    if r.status_code == 404:\n",
    "        # older naming\n",
    "        url = f\"https://www.nass.usda.gov/Research_and_Science/Cropland/Release/datasets/USDA_CDL_{year}.tif\"\n",
    "        r = requests.get(url, timeout=300)\n",
    "    r.raise_for_status()\n",
    "    with open(out, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return out\n",
    "\n",
    "CORN_CODES = {1, 3}  # 1=Corn (yellow), 3=Sweet Corn (check legend by year)\n",
    "\n",
    "def county_corn_fraction_from_cdl(tif_path: Path, counties: gpd.GeoDataFrame) -> pd.DataFrame:\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        res = []\n",
    "        for _, row in counties.iterrows():\n",
    "            geom = [row.geometry.__geo_interface__]\n",
    "            try:\n",
    "                out_image, out_transform = mask(src, geom, crop=True)\n",
    "            except Exception:\n",
    "                res.append({\"fips\": row.fips, \"corn_frac\": np.nan})\n",
    "                continue\n",
    "            data = out_image[0]\n",
    "            if data.size == 0:\n",
    "                res.append({\"fips\": row.fips, \"corn_frac\": np.nan})\n",
    "                continue\n",
    "            valid = data > 0\n",
    "            if valid.sum() == 0:\n",
    "                res.append({\"fips\": row.fips, \"corn_frac\": 0.0})\n",
    "                continue\n",
    "            corn = np.isin(data, list(CORN_CODES))\n",
    "            frac = float(corn.sum()/valid.sum())\n",
    "            res.append({\"fips\": row.fips, \"corn_frac\": frac})\n",
    "        return pd.DataFrame(res)\n",
    "\n",
    "def build_cdl_table(start_year=2008, end_year=END_YEAR):\n",
    "    rows = []\n",
    "    for y in tqdm(range(start_year, min(end_year, dt.date.today().year)+1), desc=\"CDL years\"):\n",
    "        tif = fetch_cdl_year(y)\n",
    "        df = county_corn_fraction_from_cdl(tif, MN_COUNTIES)\n",
    "        df[\"year\"]=y; df[\"cdl_corn_fraction\"]=df.pop(\"corn_frac\")\n",
    "        rows.append(df)\n",
    "    return pd.concat(rows, ignore_index=True)\n",
    "\n",
    "cdl_df = build_cdl_table()\n",
    "cdl_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece1084",
   "metadata": {},
   "source": [
    "\n",
    "## gSSURGO ‚Äî Soils (AWC, drainage, OM)\n",
    "\n",
    "We query the Soil Data Access (SDA) API to compute county-level aggregates for:\n",
    "- `gssurgo_awc` (available water capacity in top 1 m)\n",
    "- `gssurgo_drainage_idx` (ordinal drainage class)\n",
    "- (Optional) `om` (organic matter) as a cross-check\n",
    "\n",
    "> This uses an SDA SQL query and intersects with county boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c70f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sda_query(sql: str) -> pd.DataFrame:\n",
    "    url = \"https://sdmdataaccess.sc.egov.usda.gov/tabular/post.rest\"\n",
    "    headers = {\"Content-Type\":\"application/json\"}\n",
    "    payload = {\"query\": sql}\n",
    "    r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=180)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "    if \"Table\" not in js:\n",
    "        raise RuntimeError(f\"SDA response missing 'Table': {js}\")\n",
    "    return pd.DataFrame(js[\"Table\"])\n",
    "\n",
    "# We'll pull mapunits that intersect Minnesota and compute AWC from component/horizon tables.\n",
    "SQL = '''\n",
    "SELECT mu.mukey, mu.musym, mu.muname, areasymbol, areaname, mu.acres\n",
    "FROM legend lg\n",
    "JOIN mapunit mu ON mu.lkey = lg.lkey\n",
    "WHERE areasymbol LIKE 'MN%'\n",
    "'''\n",
    "try:\n",
    "    mu = sda_query(SQL)\n",
    "    mu.head()\n",
    "except Exception as e:\n",
    "    print(\"SDA query failed; check internet access.\", e)\n",
    "\n",
    "# For brevity, we show structure; full AWC computation typically aggregates from CHORIZON by thickness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e04904",
   "metadata": {},
   "source": [
    "\n",
    "## NDVI (MODIS or Sentinel-2)\n",
    "\n",
    "Two options:\n",
    "\n",
    "1) **MODIS MOD13A2 (16-day, 1 km)** via AppEEARS API (recommended for long record).  \n",
    "2) **Sentinel-2** via STAC search (10 m) for recent years.\n",
    "\n",
    "We then compute: `ndvi_mean`, `ndvi_peak`, `ndvi_peak_doy`, `ndvi_anom` per month/county.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e1c63fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No AppEEARS ZIP found and live mode disabled; keeping placeholder NDVI.\n",
      "NDVI built: (26100, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>ndvi_mean</th>\n",
       "      <th>ndvi_peak</th>\n",
       "      <th>ndvi_peak_doy</th>\n",
       "      <th>ndvi_anom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27095</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27045</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27073</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27085</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27153</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fips  year  month  ndvi_mean  ndvi_peak  ndvi_peak_doy  ndvi_anom\n",
       "0  27095  2000      1        NaN        NaN            NaN        NaN\n",
       "1  27045  2000      1        NaN        NaN            NaN        NaN\n",
       "2  27073  2000      1        NaN        NaN            NaN        NaN\n",
       "3  27085  2000      1        NaN        NaN            NaN        NaN\n",
       "4  27153  2000      1        NaN        NaN            NaN        NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==================== NDVI (MODIS via AppEEARS) ====================\n",
    "# Two paths:\n",
    "# 1) Live: set APPEEARS_TOKEN in env (or in-notebook) and set USE_APPEEARS_LIVE=True.\n",
    "# 2) Offline: place a ZIP from AppEEARS at /mnt/data/appeears_ndvi.zip (CSV bundle) and leave USE_APPEEARS_LIVE=False.\n",
    "\n",
    "import os, io, zipfile, time, json, requests, pandas as pd, numpy as np, geopandas as gpd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "APPEEARS_BASE = \"https://appeears.earthdatacloud.nasa.gov/api\"\n",
    "USE_APPEEARS_LIVE = False  # <- flip to True if you set APPEEARS_TOKEN\n",
    "\n",
    "# OPTIONAL: set token here instead of environment variable\n",
    "# APPEEARS_TOKEN = \"paste-your-token\"\n",
    "\n",
    "def _appeears_headers():\n",
    "    tok = os.environ.get(\"APPEEARS_TOKEN\", \"\").strip()\n",
    "    if not tok and \"APPEEARS_TOKEN\" in globals():\n",
    "        tok = str(APPEEARS_TOKEN).strip()\n",
    "    if not tok:\n",
    "        raise RuntimeError(\"Set APPEEARS_TOKEN to use live NDVI download.\")\n",
    "    return {\"Authorization\": f\"Bearer {tok}\"}\n",
    "\n",
    "def _appeears_post(path, payload):\n",
    "    r = requests.post(f\"{APPEEARS_BASE}{path}\", headers=_appeears_headers(), json=payload, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def _appeears_get(path):\n",
    "    r = requests.get(f\"{APPEEARS_BASE}{path}\", headers=_appeears_headers(), timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def _appeears_download_file(url, out_path):\n",
    "    with requests.get(url, headers=_appeears_headers(), stream=True, timeout=300) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(1<<20):\n",
    "                if chunk: f.write(chunk)\n",
    "\n",
    "def _mn_counties_geojson():\n",
    "    g = MN_COUNTIES[[\"fips\",\"geometry\"]].copy()\n",
    "    g[\"FID\"] = g[\"fips\"]  # AppEEARS echoes FID in CSV ‚Üí we map back to FIPS easily\n",
    "    return json.loads(g.to_json())\n",
    "\n",
    "def submit_ndvi_area_task(start_date, end_date, task_name=\"MN_NDVI_MOD13A2\"):\n",
    "    # MODIS Terra Vegetation Indices 16-Day, 1 km, Collection 6.1\n",
    "    product = \"MOD13A2.061\"\n",
    "    layers  = [\"NDVI\",\"SummaryQA\",\"DayOfYear\"]\n",
    "    payload = {\n",
    "        \"task_type\": \"area\",\n",
    "        \"task_name\": task_name,\n",
    "        \"params\": {\n",
    "            \"dates\": [{\"startDate\": start_date, \"endDate\": end_date}],\n",
    "            \"layers\": [{\"layer\": lyr, \"product\": product} for lyr in layers],\n",
    "            \"output\": {\"format\": {\"type\": \"csv\"}},\n",
    "            \"geo\": _mn_counties_geojson()\n",
    "        }\n",
    "    }\n",
    "    out = _appeears_post(\"/task\", payload)\n",
    "    return out[\"task_id\"]\n",
    "\n",
    "def wait_for_task(task_id, poll=20):\n",
    "    while True:\n",
    "        st = _appeears_get(f\"/task/{task_id}\")\n",
    "        if st.get(\"status\") in (\"done\",\"failed\"):\n",
    "            return st\n",
    "        time.sleep(poll)\n",
    "\n",
    "def download_task_bundle(task_id, out_zip=\"/mnt/data/appeears_ndvi.zip\"):\n",
    "    files = _appeears_get(f\"/bundle/{task_id}\")\n",
    "    # Prefer the CSV bundle\n",
    "    for f in files.get(\"files\", []):\n",
    "        if f.get(\"file_type\",\"\").lower() == \"csv\":\n",
    "            _appeears_download_file(f[\"url\"], out_zip)\n",
    "            return out_zip\n",
    "    # fallback: download first file\n",
    "    if files.get(\"files\"):\n",
    "        _appeears_download_file(files[\"files\"][0][\"url\"], out_zip)\n",
    "        return out_zip\n",
    "    raise RuntimeError(\"No downloadable file found in AppEEARS bundle.\")\n",
    "\n",
    "def parse_appeears_csv_zip(zip_path=\"/mnt/data/appeears_ndvi.zip\"):\n",
    "    # AppEEARS CSV has rows per feature (FID) per composite date\n",
    "    with zipfile.ZipFile(zip_path) as z:\n",
    "        cand = [n for n in z.namelist() if n.lower().endswith(\".csv\")]\n",
    "        if not cand:\n",
    "            raise FileNotFoundError(\"No CSV in AppEEARS ZIP.\")\n",
    "        with z.open(cand[0]) as f:\n",
    "            df = pd.read_csv(f)\n",
    "\n",
    "    # Normalize columns\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    if \"fid\" not in df.columns:\n",
    "        if \"feature_id\" in df.columns:\n",
    "            df[\"fid\"] = df[\"feature_id\"]\n",
    "        else:\n",
    "            raise ValueError(\"AppEEARS CSV missing FID/feature_id column.\")\n",
    "    date_col = \"date\" if \"date\" in df.columns else (\"acq_date\" if \"acq_date\" in df.columns else None)\n",
    "    if not date_col:\n",
    "        raise ValueError(\"AppEEARS CSV missing date/acq_date column.\")\n",
    "    df[\"date\"] = pd.to_datetime(df[date_col])\n",
    "    df[\"year\"] = df[\"date\"].dt.year\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "\n",
    "    # Variables: NDVI scaled 0.0001, mask with SummaryQA == 0 (best)\n",
    "    ndvi_col = next((c for c in df.columns if \"ndvi\" in c), None)\n",
    "    qa_col   = next((c for c in df.columns if \"summaryqa\" in c or c.endswith(\"_qa\")), None)\n",
    "    doy_col  = next((c for c in df.columns if \"dayofyear\" in c or c.endswith(\"_doy\")), None)\n",
    "\n",
    "    if ndvi_col is None:\n",
    "        raise ValueError(\"Could not find NDVI column in AppEEARS CSV.\")\n",
    "    df[\"ndvi\"] = pd.to_numeric(df[ndvi_col], errors=\"coerce\") * 0.0001\n",
    "    if qa_col and qa_col in df.columns:\n",
    "        qa = pd.to_numeric(df[qa_col], errors=\"coerce\")\n",
    "        df.loc[qa != 0, \"ndvi\"] = np.nan\n",
    "\n",
    "    df[\"doy\"] = pd.to_numeric(df[doy_col], errors=\"coerce\") if doy_col else df[\"date\"].dt.dayofyear\n",
    "    df[\"fips\"] = df[\"fid\"].astype(str).str.zfill(5)\n",
    "\n",
    "    # Monthly by county\n",
    "    g = df.groupby([\"fips\",\"year\",\"month\"], as_index=False)\n",
    "    ndvi_mean = g[\"ndvi\"].mean().rename(columns={\"ndvi\":\"ndvi_mean\"})\n",
    "    # Peak within month\n",
    "    peak_idx = df.groupby([\"fips\",\"year\",\"month\"])[\"ndvi\"].idxmax()\n",
    "    peaks = df.loc[peak_idx, [\"fips\",\"year\",\"month\",\"ndvi\",\"doy\"]].rename(columns={\"ndvi\":\"ndvi_peak\",\"doy\":\"ndvi_peak_doy\"})\n",
    "    out = ndvi_mean.merge(peaks, on=[\"fips\",\"year\",\"month\"], how=\"left\")\n",
    "\n",
    "    # 2001‚Äì2020 climatology per county√ómonth for anomalies\n",
    "    clim = out[(out[\"year\"]>=2001)&(out[\"year\"]<=2020)].groupby([\"fips\",\"month\"])[\"ndvi_mean\"].mean().rename(\"ndvi_mean_clim\").reset_index()\n",
    "    out = out.merge(clim, on=[\"fips\",\"month\"], how=\"left\")\n",
    "    out[\"ndvi_anom\"] = out[\"ndvi_mean\"] - out[\"ndvi_mean_clim\"]\n",
    "    out = out.drop(columns=[\"ndvi_mean_clim\"])\n",
    "    return out[[\"fips\",\"year\",\"month\",\"ndvi_mean\",\"ndvi_peak\",\"ndvi_peak_doy\",\"ndvi_anom\"]]\n",
    "\n",
    "def build_ndvi_modis_appeears(start_year=START_YEAR, end_year=END_YEAR):\n",
    "    start_date = f\"{start_year:04d}-01-01\"; end_date = f\"{end_year:04d}-12-31\"\n",
    "    tid = submit_ndvi_area_task(start_date, end_date)\n",
    "    print(\"Submitted AppEEARS NDVI task:\", tid)\n",
    "    status = wait_for_task(tid, poll=20)\n",
    "    if status.get(\"status\") != \"done\":\n",
    "        raise RuntimeError(f\"AppEEARS task failed: {status}\")\n",
    "    out_zip = download_task_bundle(tid, out_zip=\"/mnt/data/appeears_ndvi.zip\")\n",
    "    print(\"Downloaded NDVI bundle to\", out_zip)\n",
    "    return parse_appeears_csv_zip(out_zip)\n",
    "\n",
    "def ingest_appeears_ndvi_zip(zip_path=\"/mnt/data/appeears_ndvi.zip\"):\n",
    "    return parse_appeears_csv_zip(zip_path)\n",
    "\n",
    "# ==== Run it ====\n",
    "if USE_APPEEARS_LIVE:\n",
    "    ndvi_df = build_ndvi_modis_appeears(START_YEAR, END_YEAR)\n",
    "else:\n",
    "    if Path(\"/mnt/data/appeears_ndvi.zip\").exists():\n",
    "        ndvi_df = ingest_appeears_ndvi_zip(\"/mnt/data/appeears_ndvi.zip\")\n",
    "    else:\n",
    "        print(\"No AppEEARS ZIP found and live mode disabled; keeping placeholder NDVI.\")\n",
    "        ndvi_df = placeholder_ndvi_builder()\n",
    "\n",
    "print(\"NDVI built:\", ndvi_df.shape)\n",
    "display(ndvi_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d77b6ba",
   "metadata": {},
   "source": [
    "\n",
    "## Markets & Logistics Proxies\n",
    "\n",
    "- **Diesel** (EIA weekly) ‚Üí monthly mean around planting/harvest.  \n",
    "- **Ethanol plants** (EIA list) ‚Üí compute `dist_km_ethanol` from county centroid to nearest plant.  \n",
    "- **Basis** (USDA AMS bids; optional) ‚Üí join nearest terminal to county; use May‚ÄìJune mean.\n",
    "\n",
    "> We include diesel and ethanol distance here. Basis collection varies by terminal and may require custom scrapers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8cbd0430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tngzj\\AppData\\Local\\Temp\\ipykernel_38024\\1730679609.py:25: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroids[\"centroid\"] = centroids.geometry.centroid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>dist_km_ethanol</th>\n",
       "      <th>nearest_ethanol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27095</td>\n",
       "      <td>127.415615</td>\n",
       "      <td>Bushmills Ethanol - Atwater, MN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27045</td>\n",
       "      <td>0.445526</td>\n",
       "      <td>POET - Preston, MN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27073</td>\n",
       "      <td>109.682327</td>\n",
       "      <td>Bushmills Ethanol - Atwater, MN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27085</td>\n",
       "      <td>53.088949</td>\n",
       "      <td>Bushmills Ethanol - Atwater, MN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27153</td>\n",
       "      <td>104.927353</td>\n",
       "      <td>Bushmills Ethanol - Atwater, MN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fips  dist_km_ethanol                  nearest_ethanol\n",
       "0  27095       127.415615  Bushmills Ethanol - Atwater, MN\n",
       "1  27045         0.445526               POET - Preston, MN\n",
       "2  27073       109.682327  Bushmills Ethanol - Atwater, MN\n",
       "3  27085        53.088949  Bushmills Ethanol - Atwater, MN\n",
       "4  27153       104.927353  Bushmills Ethanol - Atwater, MN"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from math import radians, sin, cos, asin, sqrt\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    dlat = radians(lat2-lat1); dlon = radians(lon2-lon1)\n",
    "    a = sin(dlat/2)**2 + cos(radians(lat1))*cos(radians(lat2))*sin(dlon/2)**2\n",
    "    return 2*R*asin(sqrt(a))\n",
    "\n",
    "def fetch_eia_ethanol_plants() -> pd.DataFrame:\n",
    "    # Example CSV (public snapshot). Replace with EIA authoritative list if available.\n",
    "    # For now, we provide a small MN-centric list as a starter.\n",
    "    plants = [\n",
    "        {\"name\":\"Guardian Energy - Janesville, MN\",\"lat\":44.12,\"lon\":-93.71},\n",
    "        {\"name\":\"POET - Preston, MN\",\"lat\":43.67,\"lon\":-92.09},\n",
    "        {\"name\":\"Green Plains - Fairmont, MN\",\"lat\":43.63,\"lon\":-94.46},\n",
    "        {\"name\":\"Bushmills Ethanol - Atwater, MN\",\"lat\":45.13,\"lon\":-94.79},\n",
    "        {\"name\":\"POET - Glenville, MN\",\"lat\":43.56,\"lon\":-93.34},\n",
    "    ]\n",
    "    return pd.DataFrame(plants)\n",
    "\n",
    "def compute_ethanol_distance():\n",
    "    plants = fetch_eia_ethanol_plants()\n",
    "    rows = []\n",
    "    centroids = MN_COUNTIES.copy()\n",
    "    centroids[\"centroid\"] = centroids.geometry.centroid\n",
    "    for _, r in centroids.iterrows():\n",
    "        clat = r.centroid.y; clon = r.centroid.x\n",
    "        dmin = 1e9; which = None\n",
    "        for _, p in plants.iterrows():\n",
    "            d = haversine_km(clat, clon, p.lat, p.lon)\n",
    "            if d < dmin:\n",
    "                dmin = d; which = p[\"name\"]\n",
    "        rows.append({\"fips\": r.fips, \"dist_km_ethanol\": dmin, \"nearest_ethanol\": which})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "ethanol_df = compute_ethanol_distance()\n",
    "ethanol_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7edbb97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>diesel_usd_gal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>2.530093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1994</td>\n",
       "      <td>2</td>\n",
       "      <td>2.597270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1994</td>\n",
       "      <td>3</td>\n",
       "      <td>2.655512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1994</td>\n",
       "      <td>4</td>\n",
       "      <td>2.711496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1994</td>\n",
       "      <td>5</td>\n",
       "      <td>2.770678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  diesel_usd_gal\n",
       "0  1994      1        2.530093\n",
       "1  1994      2        2.597270\n",
       "2  1994      3        2.655512\n",
       "3  1994      4        2.711496\n",
       "4  1994      5        2.770678"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def fetch_eia_diesel_weekly() -> pd.DataFrame:\n",
    "    # EIA weekly U.S. No. 2 Diesel Retail Prices (Dollars per Gallon) ‚Äî national series\n",
    "    # CSV download:\n",
    "    url = \"https://www.eia.gov/dnav/pet/hist_xls/RWTCd.htm\"  # Placeholder; consider EIA API for JSON with API key.\n",
    "    # For illustration, we will create a stub series (you should replace with EIA API call).\n",
    "    dates = pd.date_range(\"1994-01-03\",\"2025-10-20\", freq=\"W-MON\")\n",
    "    vals = np.clip(2.5 + 0.5*np.sin(np.linspace(0, 50, len(dates))), 1.5, 6.0)\n",
    "    return pd.DataFrame({\"date\":dates, \"diesel_usd_gal\": vals})\n",
    "\n",
    "def diesel_monthly_mean(diesel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    diesel_df[\"year\"] = diesel_df[\"date\"].dt.year\n",
    "    diesel_df[\"month\"] = diesel_df[\"date\"].dt.month\n",
    "    out = diesel_df.groupby([\"year\",\"month\"]).agg(diesel_usd_gal=(\"diesel_usd_gal\",\"mean\")).reset_index()\n",
    "    return out\n",
    "\n",
    "diesel_df = diesel_monthly_mean(fetch_eia_diesel_weekly())\n",
    "diesel_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13bebd7c-c98e-4616-8d7a-8dd90ae1981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diesel_df.to_csv('diesel_dist.csv', index=False) \n",
    "ethanol_df.to_csv('ethanol_dist.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1d7644",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Engineering & Merge to Minimal Schema\n",
    "\n",
    "We compute:\n",
    "- `heat_days_35c` (approximate from PRISM monthly Tmax via exceedance proxy)\n",
    "- `vpd_proxy` (simple Clausius‚ÄìClapeyron-based monthly proxy)\n",
    "- `moisture_deficit = prism_prcp_mm - gldas_et_mm`\n",
    "- Label horizons and keep only rows up to each horizon month per year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e8e42c1-3387-4897-96e8-350b3bf14618",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request for url: https://quickstats.nass.usda.gov/api/api_GET/?key=606153B9-DDAD-30FB-88EE-5BCC60DEE678&source_desc=SURVEY&sector_desc=CROPS&group_desc=FIELD+CROPS&commodity_desc=CORN&statisticcat_desc=PRICE+RECEIVED&unit_desc=%24+%2F+BU&agg_level_desc=STATE&state_alpha=MN&reference_period_desc=MONTH&year__GE=1999&year__LE=2024&format=JSON",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 133\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Build\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m     corn_price_df \u001b[38;5;241m=\u001b[39m build_corn_price_features(START_YEAR, END_YEAR)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# Fallback if START_YEAR/END_YEAR not defined in the notebook yet\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     corn_price_df \u001b[38;5;241m=\u001b[39m build_corn_price_features(\u001b[38;5;241m2000\u001b[39m, _dt\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39myear)\n",
      "Cell \u001b[1;32mIn[59], line 113\u001b[0m, in \u001b[0;36mbuild_corn_price_features\u001b[1;34m(start_year, end_year)\u001b[0m\n\u001b[0;32m    110\u001b[0m fut_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(fut_rows)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# MN cash price monthly (include prior year for harvest window)\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m cash \u001b[38;5;241m=\u001b[39m mn_cash_prices_monthly(start_year\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, end_year)\n\u001b[0;32m    114\u001b[0m preplant \u001b[38;5;241m=\u001b[39m cash[cash[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(PREPLANT_MONTHS)]\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmn_cash_usd_bu\u001b[39m\u001b[38;5;124m\"\u001b[39m] \\\n\u001b[0;32m    115\u001b[0m            \u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmn_cash_preplant_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Prior-year harvest (Sep‚ÄìNov of y-1 attributed to year y)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[59], line 82\u001b[0m, in \u001b[0;36mmn_cash_prices_monthly\u001b[1;34m(start_year, end_year)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"MN cash price received ($/bu), monthly.\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_desc\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSURVEY\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msector_desc\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCROPS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear__LE\u001b[39m\u001b[38;5;124m\"\u001b[39m: end_year,\n\u001b[0;32m     81\u001b[0m }\n\u001b[1;32m---> 82\u001b[0m df \u001b[38;5;241m=\u001b[39m quickstats_query(params)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmn_cash_usd_bu\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[59], line 52\u001b[0m, in \u001b[0;36mquickstats_query\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     50\u001b[0m p \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m: QS_KEY, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJSON\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     51\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(QS_BASE, params\u001b[38;5;241m=\u001b[39mp, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     53\u001b[0m data \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     54\u001b[0m rows \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1021\u001b[0m     )\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://quickstats.nass.usda.gov/api/api_GET/?key=606153B9-DDAD-30FB-88EE-5BCC60DEE678&source_desc=SURVEY&sector_desc=CROPS&group_desc=FIELD+CROPS&commodity_desc=CORN&statisticcat_desc=PRICE+RECEIVED&unit_desc=%24+%2F+BU&agg_level_desc=STATE&state_alpha=MN&reference_period_desc=MONTH&year__GE=1999&year__LE=2024&format=JSON"
     ]
    }
   ],
   "source": [
    "# ================= Corn price features (no leakage) =================\n",
    "# Dec corn futures (contract for year t) via yfinance + MN cash price via USDA QuickStats\n",
    "# Outputs per-year features:\n",
    "#   corn_fut_dec_preplant_avg, corn_price_volatility_preplant,\n",
    "#   mn_cash_preplant_avg, corn_basis_state_preplant, mn_cash_harvest_prior\n",
    "\n",
    "import os, pandas as pd, numpy as np, requests\n",
    "from datetime import datetime as _dt\n",
    "\n",
    "# ---- Config ----\n",
    "USE_YFINANCE = True  # set False to skip futures if yfinance isn't installed\n",
    "PREPLANT_MONTHS = [1,2,3,4]   # Jan‚ÄìApr\n",
    "HARVEST_MONTHS  = [9,10,11]   # Sep‚ÄìNov (prior year, attributed to year t)\n",
    "QS_BASE = \"https://quickstats.nass.usda.gov/api/api_GET/\"\n",
    "QS_KEY  = \"606153B9-DDAD-30FB-88EE-5BCC60DEE678\"\n",
    "\n",
    "# Try import yfinance (only needed for futures)\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except Exception:\n",
    "    yf = None\n",
    "    USE_YFINANCE = False\n",
    "\n",
    "def _dec_contract_symbols(year: int):\n",
    "    \"\"\"Candidate Yahoo symbols for December corn contract of 'year' (ZC Dec).\"\"\"\n",
    "    yy = year % 100\n",
    "    return [f\"ZCZ{yy}.CBT\", f\"ZCZ{yy}\", \"ZC=F\"]  # last is continuous fallback\n",
    "\n",
    "def _fetch_dec_futures_preplant(year: int) -> pd.Series:\n",
    "    \"\"\"Jan‚ÄìApr daily settles for December contract of 'year' as a Series.\"\"\"\n",
    "    if not USE_YFINANCE or yf is None:\n",
    "        return pd.Series(dtype=float)\n",
    "    start = f\"{year}-01-01\"; end = f\"{year}-04-30\"\n",
    "    for sym in _dec_contract_symbols(year):\n",
    "        try:\n",
    "            df = yf.download(sym, start=start, end=end, progress=False, auto_adjust=False, actions=False)\n",
    "            if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "                px = df.get(\"Adj Close\", df.get(\"Close\"))\n",
    "                px = pd.to_numeric(px, errors=\"coerce\").dropna()\n",
    "                if not px.empty:\n",
    "                    return px\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.Series(dtype=float)\n",
    "\n",
    "def quickstats_query(params: dict) -> pd.DataFrame:\n",
    "    \"\"\"USDA NASS QuickStats fetch ‚Üí DataFrame with lowercase columns and numeric 'value'.\"\"\"\n",
    "    if not QS_KEY:\n",
    "        return pd.DataFrame()\n",
    "    p = {\"key\": QS_KEY, **params, \"format\": \"JSON\"}\n",
    "    r = requests.get(QS_BASE, params=p, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    rows = data.get(\"data\", [])\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    if \"value\" in df.columns:\n",
    "        df[\"value\"] = (\n",
    "            df[\"value\"].astype(str).str.replace(\",\", \"\", regex=False)\n",
    "            .replace({\"(d)\": np.nan, \"(z)\": 0}, regex=True)\n",
    "        )\n",
    "        df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def mn_cash_prices_monthly(start_year: int, end_year: int) -> pd.DataFrame:\n",
    "    \"\"\"MN cash price received ($/bu), monthly.\"\"\"\n",
    "    params = {\n",
    "        \"source_desc\": \"SURVEY\",\n",
    "        \"sector_desc\": \"CROPS\",\n",
    "        \"group_desc\": \"FIELD CROPS\",\n",
    "        \"commodity_desc\": \"CORN\",\n",
    "        \"statisticcat_desc\": \"PRICE RECEIVED\",\n",
    "        \"unit_desc\": \"$ / BU\",\n",
    "        \"agg_level_desc\": \"STATE\",\n",
    "        \"state_alpha\": \"MN\",\n",
    "        \"reference_period_desc\": \"MONTH\",\n",
    "        \"year__GE\": start_year,\n",
    "        \"year__LE\": end_year,\n",
    "    }\n",
    "    df = quickstats_query(params)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"year\",\"month\",\"mn_cash_usd_bu\"])\n",
    "    # Parse month from 'time_period' or 'period'\n",
    "    mon_map = {m.upper(): i for i, m in enumerate([\"\",\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"])}\n",
    "    if \"time_period\" in df.columns:\n",
    "        mtxt = df[\"time_period\"].astype(str).str[:3].str.upper()\n",
    "    elif \"period\" in df.columns:\n",
    "        mtxt = df[\"period\"].astype(str).str[:3].str.upper()\n",
    "    else:\n",
    "        mtxt = np.nan\n",
    "    df[\"month\"] = mtxt.map(mon_map)\n",
    "    out = df[[\"year\",\"month\",\"value\"]].dropna()\n",
    "    out[\"year\"] = pd.to_numeric(out[\"year\"], errors=\"coerce\").astype(int)\n",
    "    out[\"month\"] = pd.to_numeric(out[\"month\"], errors=\"coerce\").astype(int)\n",
    "    out = out.rename(columns={\"value\":\"mn_cash_usd_bu\"})\n",
    "    return out\n",
    "\n",
    "def build_corn_price_features(start_year: int, end_year: int) -> pd.DataFrame:\n",
    "    # Futures: Jan‚ÄìApr stats for Dec contract of year t\n",
    "    fut_rows = []\n",
    "    for y in range(start_year, end_year+1):\n",
    "        px = _fetch_dec_futures_preplant(y)\n",
    "        fut_rows.append({\n",
    "            \"year\": y,\n",
    "            \"corn_fut_dec_preplant_avg\": float(px.mean()) if not px.empty else np.nan,\n",
    "            \"corn_price_volatility_preplant\": float(px.std(ddof=0)) if not px.empty else np.nan\n",
    "        })\n",
    "    fut_df = pd.DataFrame(fut_rows)\n",
    "\n",
    "    # MN cash price monthly (include prior year for harvest window)\n",
    "    cash = mn_cash_prices_monthly(start_year-1, end_year)\n",
    "    preplant = cash[cash[\"month\"].isin(PREPLANT_MONTHS)].groupby(\"year\")[\"mn_cash_usd_bu\"] \\\n",
    "               .mean().rename(\"mn_cash_preplant_avg\").reset_index()\n",
    "\n",
    "    # Prior-year harvest (Sep‚ÄìNov of y-1 attributed to year y)\n",
    "    harvest = cash[cash[\"month\"].isin(HARVEST_MONTHS)].copy()\n",
    "    harvest[\"year\"] = harvest[\"year\"] + 1\n",
    "    harvest_prior = harvest.groupby(\"year\")[\"mn_cash_usd_bu\"] \\\n",
    "                     .mean().rename(\"mn_cash_harvest_prior\").reset_index()\n",
    "\n",
    "    out = fut_df.merge(preplant, on=\"year\", how=\"left\").merge(harvest_prior, on=\"year\", how=\"left\")\n",
    "    out[\"corn_basis_state_preplant\"] = out[\"mn_cash_preplant_avg\"] - out[\"corn_fut_dec_preplant_avg\"]\n",
    "    # Convenience month for merging; you can also drop 'month' and merge on year only\n",
    "    out[\"month\"] = 4\n",
    "    cols = [\"year\",\"month\",\"corn_fut_dec_preplant_avg\",\"corn_price_volatility_preplant\",\n",
    "            \"mn_cash_preplant_avg\",\"corn_basis_state_preplant\",\"mn_cash_harvest_prior\"]\n",
    "    return out[cols].sort_values(\"year\")\n",
    "\n",
    "# Build\n",
    "try:\n",
    "    corn_price_df = build_corn_price_features(START_YEAR, END_YEAR)\n",
    "except NameError:\n",
    "    # Fallback if START_YEAR/END_YEAR not defined in the notebook yet\n",
    "    corn_price_df = build_corn_price_features(2000, _dt.now().year)\n",
    "\n",
    "print(\"corn_price_df:\", corn_price_df.shape)\n",
    "display(corn_price_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb083196-bffe-41fd-a00d-8061a3412b44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
