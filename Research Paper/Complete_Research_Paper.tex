\documentclass[10pt,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{balance}
\usepackage{flushend}
\usepackage{abstract}
\usepackage{url}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{siunitx}
\usepackage{fixmath}
\usepackage{adjustbox}

% Standard research paper margins and spacing
\geometry{
    top=0.75in,
    bottom=0.75in,
    left=0.6in,
    right=0.6in,
    columnsep=0.25in
}

% Line spacing - standard for research papers
\renewcommand{\baselinestretch}{0.98}
\setlength{\parskip}{0.5ex}
\setlength{\parindent}{10pt}

% Itemize spacing for two-column
\setlist{nosep, leftmargin=*}
\setlist[itemize]{topsep=2pt, itemsep=1pt}
\setlist[enumerate]{topsep=2pt, itemsep=1pt}

% Custom reference commands (best practice: keep references whole)
\newcommand{\refalg}[1]{Algorithm~\ref{#1}}
\newcommand{\refapp}[1]{Appendix~\ref{#1}}
\newcommand{\refchap}[1]{Chapter~\ref{#1}}
\newcommand{\refeq}[1]{Equation~\ref{#1}}
\newcommand{\reffig}[1]{Figure~\ref{#1}}
\newcommand{\refsec}[1]{Section~\ref{#1}}
\newcommand{\reftab}[1]{Table~\ref{#1}}

% Mathematical notation commands
\renewcommand{\vec}[1]{\mathbold{#1}}
\newcommand{\mat}[1]{\mathbold{#1}}

% siunitx configuration
\sisetup{
    group-separator = {,},
    group-minimum-digits = 4,
    round-mode = figures,
    round-precision = 3,
    per-mode = fraction
}

% Abstract formatting for two-column
\renewcommand{\abstractname}{\small\textbf{Abstract}}

% Section heading formatting for two-column
\makeatletter
\renewcommand\section{\@startsection{section}{1}{\z@}%
  {-3.5ex \@plus -1ex \@minus -.2ex}%
  {2.3ex \@plus.2ex}%
  {\normalfont\large\bfseries}}
\renewcommand\subsection{\@startsection{subsection}{2}{\z@}%
  {-3.25ex\@plus -1ex \@minus -.2ex}%
  {1.5ex \@plus .2ex}%
  {\normalfont\normalsize\bfseries}}
\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}%
  {-3.25ex\@plus -1ex \@minus -.2ex}%
  {1.5ex \@plus .2ex}%
  {\normalfont\normalsize\itshape}}
\makeatother

\title{\large\textbf{A Machine Learning Benchmark for Predicting County-Level Corn Production in Minnesota Using Multi-Source Satellite, Environmental, and Economic Data}}
\author{\small [Author Name]}
\date{\small \today}

\begin{document}

\maketitle

\begin{abstract}
\small
This research presents a comprehensive machine learning benchmark framework for predicting county-level corn production in Minnesota by integrating multiple heterogeneous data sources, including satellite-derived environmental variables from the Global Land Data Assimilation System (GLDAS), economic indicators, fuel prices, transportation infrastructure data, and precipitation data from the Parameter-elevation Regressions on Independent Slopes Model (PRISM).
Eight machine learning algorithms were systematically evaluated across different complexity levels: Polynomial Regression, Support Vector Machine (SVM), Random Forest, XGBoost, LightGBM, TabNet, Temporal Neural Networks (LSTM), and Temporal Convolutional Networks (TCN).
The study employed rigorous preprocessing including temporal train-test splitting (2000-2019 for training, 2020-2022 for testing), multi-strategy imputation, comprehensive feature engineering (66 features), and robust scaling.
The LightGBM model achieved superior performance with an R² of \num{0.9930} and root mean squared error (RMSE) of \num{1137001} bushels, explaining 99.30\% of variance in corn production.
Feature importance analysis revealed that agricultural context features (\texttt{corn\_acres\_planted}, yield per acre) dominated with 48.2\% importance, followed by economic indicators (\texttt{fuel\_cost\_proxy}, revenue metrics) and environmental variables.
The results demonstrate that gradient boosting algorithms, particularly LightGBM, significantly outperform traditional ensemble methods and deep learning architectures for this multi-source tabular regression task.
Practical implications include enhanced supply chain planning, risk mitigation for agricultural stakeholders, and optimized resource allocation through accurate yield forecasting.

\textbf{Keywords:} Agricultural yield prediction, machine learning, satellite remote sensing, LightGBM, multi-source data integration, GLDAS, PRISM, economic indicators, supply chain optimization
\end{abstract}

\section{Introduction}

Agriculture represents a cornerstone of the United States economy, with corn production playing a particularly critical role in national food security, biofuel production, and economic stability \cite{vieira2023corn}.
The agricultural sector contributes approximately \SI{1.4}{\percent} to the U.S. gross domestic product, and corn specifically accounts for over \SI{15}{\percent} of total agricultural value, representing billions of dollars in economic activity annually.
Beyond direct economic impact, corn production influences global food prices, biofuel markets, and supply chain dynamics across multiple industries including livestock feed, ethanol production, and food processing.
Accurate yield forecasting enables stakeholders including farmers, agricultural cooperatives, commodity traders, policymakers, and supply chain managers to make informed decisions regarding planting strategies, resource allocation, pricing, risk management, and market planning.

Traditional forecasting methods for agricultural yield prediction rely primarily on historical averages, field surveys, and expert knowledge, approaches that are fundamentally limited in their ability to capture spatial-temporal variability and adapt to changing conditions.
Field surveys provide valuable ground truth data but are time-consuming, expensive, and limited in spatial coverage.
Historical averages fail to account for inter-annual variability driven by climate extremes, technological advances, economic conditions, and evolving agricultural practices.
These limitations become particularly problematic given increasing climate variability, where extreme weather events such as droughts, floods, and temperature anomalies create significant year-to-year production fluctuations that historical averages cannot anticipate.
The integration of satellite remote sensing data with economic and agricultural context data, combined with advanced machine learning techniques, offers a promising data-driven approach for yield prediction at county and regional scales that can overcome these traditional limitations.

Satellite remote sensing has revolutionized agricultural monitoring by providing spatially comprehensive, temporally consistent, and cost-effective observations of land surface conditions.
The Global Land Data Assimilation System (GLDAS), developed by NASA and partner agencies, synthesizes satellite and ground-based observations to produce high-quality land surface variables including soil moisture, temperature, evapotranspiration, and precipitation at regional to global scales.
Similarly, the Parameter-elevation Regressions on Independent Slopes Model (PRISM) provides high-resolution gridded climate data that captures topographic influences on precipitation and temperature patterns.
These satellite-derived environmental variables capture critical factors influencing crop growth including water availability, thermal stress, and growing season conditions that traditional methods cannot comprehensively assess.

However, agricultural yield is not determined solely by environmental conditions.
Economic factors including fuel costs, commodity prices, government support programs, and market access significantly influence planting decisions, input intensity, and ultimately production levels.
Transportation infrastructure, particularly proximity to processing facilities such as ethanol plants, affects market access and profitability, thereby influencing production decisions.
Furthermore, technological advances, changing agricultural practices, and policy shifts create temporal trends that historical averages cannot capture.
A comprehensive yield prediction framework must therefore integrate multiple heterogeneous data sources beyond satellite-derived environmental variables to account for the multi-faceted nature of agricultural production.

This research extends previous work by developing a comprehensive benchmark framework that systematically integrates five primary data sources: (1) GLDAS satellite-derived environmental variables, (2) USDA corn harvest and planted acres data, (3) economic indicators from USDA Economic Research Service, (4) diesel fuel prices as proxies for operational costs, and (5) PRISM precipitation and temperature data.
The study focuses specifically on Minnesota counties (FIPS codes 27001-27171) over the period 2000-2022, providing a rich temporal and spatial dataset encompassing \num{87} counties over \num{23} years.
By systematically comparing eight machine learning algorithms across different complexity levels, this study identifies optimal modeling approaches for multi-source agricultural yield prediction and provides actionable insights for feature importance and data collection priorities.

The primary research objectives are threefold: (1) develop a comprehensive benchmark framework comparing multiple machine learning algorithms (Polynomial Regression, SVM, Random Forest, XGBoost, LightGBM, TabNet, LSTM, and TCN) for agricultural yield prediction, (2) identify optimal modeling approaches that balance performance, complexity, and interpretability for practical deployment, and (3) analyze feature importance to understand which variables drive predictions and provide guidance for data collection priorities and model deployment strategies.
The benchmark framework employs rigorous methodology including temporal train-test splitting to prevent data leakage, comprehensive feature engineering creating \num{66} informative features, multi-strategy imputation for handling missing data in heterogeneous sources, and robust scaling to handle outliers and distributional differences.

This study contributes to the agricultural yield prediction literature by demonstrating that gradient boosting methods, particularly LightGBM, achieve superior performance (R² = \num{0.9930}) compared to traditional ensemble methods and deep learning architectures for multi-source tabular regression tasks.
The feature importance analysis reveals critical insights about feature redundancy and compensatory mechanisms, demonstrating that models can maintain strong predictive performance even when primary features are unavailable, through alternative predictive pathways leveraging economic and environmental proxies.
These findings have practical implications for supply chain planning, risk mitigation, and resource optimization in agricultural systems, where accurate yield forecasting enables proactive decision-making and enhanced resilience to climate and economic variability.

\section{Literature Review}

The integration of satellite remote sensing data with machine learning techniques for agricultural yield prediction has emerged as a rapidly evolving field, driven by advances in remote sensing technology, increased computational capabilities, and growing recognition of agriculture's critical role in global food security and economic stability.
This literature review organizes the relevant research into thematic areas: remote sensing modalities and data sources, challenges in satellite-based agricultural monitoring, machine learning applications for yield prediction, and trends toward multi-source data integration.

\subsection{Remote Sensing Modalities for Agricultural Monitoring}

Satellite remote sensing for agricultural applications encompasses multiple sensor modalities, each providing unique insights into crop conditions and environmental factors influencing yield.
Synthetic Aperture Radar (SAR) systems, operating at microwave frequencies, offer all-weather capability and sensitivity to soil moisture, vegetation structure, and biomass \cite{sentinel1,roy2025sentinel1,palsar1997}.
SAR polarimetry, which analyzes the polarization state of reflected signals, enables more sophisticated characterization of agricultural targets compared to single-polarization SAR \cite{karachristos2024polsar}.
Polarimetric decompositions such as Freeman-Durden and Cloude-Pottier decompositions extract physical scattering mechanisms including surface scattering from bare soil, double-bounce scattering from vertical structures, and volume scattering from vegetation canopies \cite{karachristos2024polsar}.
These decompositions have been successfully applied to corn yield prediction, where volume scattering correlates with vegetation biomass and crop health.
However, SAR data interpretation requires sophisticated signal processing and calibration procedures, and vegetation interference can complicate soil moisture estimation during growing seasons \cite{roy2025sentinel1}.

Optical imagery from sensors such as Landsat, MODIS, Sentinel-2, and commercial satellites provides complementary information through vegetation indices including Normalized Difference Vegetation Index (NDVI), Enhanced Vegetation Index (EVI), and Green Normalized Difference Vegetation Index (GNDVI) \cite{kayad2019sentinel2}.
These indices capture photosynthetic activity, chlorophyll content, and vegetation vigor, which correlate with crop health and potential yield.
Time series of vegetation indices throughout the growing season enable phenological monitoring, identification of stress events, and yield estimation through relationships with accumulated vegetation metrics.
However, optical sensors face critical limitations including cloud contamination, atmospheric interference, and limited revisit frequency, particularly problematic for agricultural monitoring where timely observations during critical growth stages are essential \cite{singh2025cloud}.

Multi-spectral and hyperspectral sensors expand beyond traditional RGB and near-infrared bands to capture narrow spectral bands sensitive to specific biochemical and biophysical properties.
Hyperspectral imaging enables estimation of leaf chlorophyll content, nitrogen status, water stress indicators, and canopy structure parameters that directly relate to yield potential.
However, hyperspectral data requires sophisticated preprocessing including atmospheric correction, radiometric calibration, and dimensionality reduction, and computational costs limit large-scale applications.
Thermal infrared sensors measure canopy temperature, which correlates with evapotranspiration rates and water stress, providing complementary information to optical vegetation indices.

Recent advances in data fusion techniques combine multiple sensor modalities to leverage complementary strengths.
For example, SAR data's all-weather capability compensates for optical imagery's cloud limitations, while optical data provides more direct vegetation information than SAR.
Machine learning approaches have successfully integrated multi-modal remote sensing data, demonstrating improved yield prediction accuracy compared to single-modality approaches.

\subsection{Challenges in Satellite-Based Agricultural Monitoring}

Cloud masking represents one of the most critical challenges for optical satellite imagery in agricultural applications.
Cloud contamination can obscure critical portions of the growing season, particularly problematic during key phenological stages when crop conditions strongly influence final yield \cite{singh2025cloud}.
Various cloud detection algorithms have been developed including threshold-based methods, machine learning classifiers, and deep learning approaches, but none achieve perfect accuracy, and residual cloud contamination can degrade vegetation index accuracy \cite{singh2025cloud}.
Temporal compositing techniques including maximum value composites and median composites help mitigate cloud impacts but at the cost of temporal resolution.

Vegetation interference creates challenges for both SAR and optical sensors.
During peak growing seasons, dense crop canopies obscure underlying soil conditions, making it difficult to extract soil moisture from SAR backscatter or assess soil properties from optical imagery.
This interference is particularly problematic for early-season monitoring where soil conditions strongly influence planting decisions and initial crop establishment.
Phenological stage-dependent modeling approaches that adjust interpretation based on crop development stage help address this challenge but require accurate crop calendar information.

Scale mismatches between satellite pixel resolution, agricultural field boundaries, and county-level aggregation create challenges for yield prediction applications.
Medium-resolution sensors such as MODIS (\num{250}--\num{500} meter pixels) and Landsat (\num{30} meter pixels) may not fully resolve individual fields, leading to mixed pixels containing multiple crops, soil, and non-agricultural land cover.
High-resolution commercial satellites provide sub-meter resolution but at substantially higher cost and limited spatial coverage.
County-level aggregation, common in yield prediction applications for policy and planning purposes, requires spatial integration that may obscure important sub-county variability while providing data at scales relevant for decision-making.

Atmospheric effects including aerosols, water vapor, and atmospheric scattering distort satellite observations, requiring sophisticated atmospheric correction procedures that may introduce uncertainties.
Radiometric calibration across different sensors and temporal periods ensures data consistency but remains challenging given sensor-specific characteristics and calibration drift over time.
Geometric registration errors, particularly when fusing multiple data sources, can create misalignment artifacts that degrade prediction accuracy.

\subsection{Machine Learning Applications for Yield Prediction}

Machine learning has revolutionized agricultural yield prediction by enabling data-driven modeling of complex, non-linear relationships between satellite observations, environmental conditions, and crop yields.
Traditional statistical approaches including linear regression, time series models, and econometric models provided early frameworks but often failed to capture the complex interactions between multiple factors influencing yield.
Machine learning approaches automatically discover these interactions and adapt to data characteristics without requiring explicit specification of functional forms.

Random Forest (RF) has emerged as one of the most widely applied machine learning algorithms for agricultural yield prediction, valued for its interpretability, robustness to outliers, and ability to handle mixed data types \cite{prasetyo2024random}.
RF ensembles of decision trees capture non-linear relationships and feature interactions while providing feature importance metrics that aid interpretation.
Numerous studies have demonstrated RF's effectiveness for county-level and field-level yield prediction using satellite-derived vegetation indices, weather data, and soil properties \cite{prasetyo2024random}.
However, RF's independent tree structure prevents sequential learning from previous errors, and bagging-based ensemble approaches may miss optimal feature combinations that boosting methods can discover.

Convolutional Neural Networks (CNNs) have been adapted for agricultural applications by treating spatial satellite imagery as image inputs, enabling automatic feature extraction from spatial patterns \cite{shahhosseini2021cnn}.
CNNs applied to multi-temporal stacks of satellite imagery can learn phenological patterns, spatial heterogeneity, and stress indicators that correlate with yield \cite{shahhosseini2021cnn,nevavuori2020crop}.
However, CNNs require large amounts of training data and may overfit on limited agricultural datasets.
The spatial structure assumptions of CNNs may not align with agricultural data where tabular features (climate, economic, soil properties) dominate alongside spatial patterns.

Long Short-Term Memory (LSTM) networks and Recurrent Neural Networks (RNN) have been applied to capture temporal dynamics in satellite time series data throughout growing seasons.
LSTM's ability to model long-term dependencies makes it well-suited for capturing the influence of early-season conditions on final yield, where stress events during critical growth stages have disproportionate impacts.
However, LSTM architectures assume sequential temporal structure, which may not align with agricultural data where independent samples (counties, years) are more common than true time series.
The sequence length assumptions of LSTM may limit applicability to tabular regression tasks with rich feature sets but limited temporal sequences per sample.

Ensemble methods combining multiple algorithms have demonstrated improved accuracy compared to individual models.
Stacking ensembles that train meta-learners on base model predictions can capture complementary strengths of different algorithms.
However, ensembles increase computational complexity and may provide marginal improvements that do not justify added complexity for deployment applications.
Gradient boosting methods including XGBoost \cite{chen2016xgboost} and LightGBM \cite{ke2017lightgbm} represent ensemble approaches that sequentially improve predictions by learning from previous errors, demonstrating superior performance for tabular regression tasks compared to bagging-based approaches.

\subsection{Trends Toward Multi-Source Data Integration}

Recent research trends emphasize integration of multiple heterogeneous data sources beyond satellite-derived environmental variables, recognizing that agricultural yield results from complex interactions between environmental conditions, economic factors, technological advances, and policy influences \cite{desloires2024corn,ji2022corn}.
Economic indicators including commodity prices, fuel costs, government support programs, and market access factors significantly influence planting decisions, input intensity, and production levels that environmental variables alone cannot capture.
Studies incorporating economic data alongside satellite observations demonstrate improved prediction accuracy and more realistic modeling of farmer decision-making \cite{desloires2024corn}.

Soil property databases including the gSSURGO (Gridded Soil Survey Geographic) database provide important context for yield prediction, where soil characteristics including texture, organic matter, drainage, and fertility create spatial heterogeneity that environmental variables may not fully capture.
Crop-specific soil suitability indices and land capability classes help interpret yield variability across regions with similar environmental conditions.
Integration of soil data with satellite observations and climate data provides more comprehensive characterization of growing conditions.

Crop-specific datasets including the USDA Cropland Data Layer (CDL) provide annual crop type classification, enabling crop-specific modeling and identification of crop-specific yield drivers.
Market and logistics data including futures prices, transportation costs, and processing facility locations capture economic incentives and constraints that influence production decisions.
Recent studies incorporating these multi-source datasets demonstrate that integration improves prediction accuracy compared to single-source approaches, though computational and data management challenges increase with complexity.

Feature engineering from multi-source data creates valuable interaction terms, ratios, and derived metrics that capture relationships not present in raw variables.
For example, precipitation efficiency metrics (soil moisture per unit precipitation) normalize environmental conditions across regions with different baseline precipitation patterns.
Economic interaction features combining revenue and production data provide scale-normalized metrics that aid interpretation.
Temporal features capturing long-term trends, seasonal patterns, and anomalies help models distinguish between short-term variability and structural changes.

\subsection{Research Gaps and Contributions}

Despite advances in multi-source data integration and machine learning applications, several research gaps remain.
Most studies focus on single algorithms or limited comparisons, lacking comprehensive benchmarks across complexity levels.
Feature importance analysis often lacks depth in understanding compensatory mechanisms when primary features are unavailable.
Practical deployment guidance for balancing model complexity, performance, and interpretability remains limited.
Generalizability across different crops, regions, and temporal periods requires further investigation.

This research contributes to addressing these gaps by: (1) providing a comprehensive benchmark comparing eight algorithms across low, medium, and high complexity levels, (2) conducting dual-configuration analysis demonstrating feature redundancy and compensatory mechanisms when primary features are excluded, (3) providing detailed feature importance analysis explaining why certain features gain prominence when others are removed, (4) establishing robust methodology including temporal splitting, multi-strategy imputation, and comprehensive feature engineering, and (5) offering practical deployment recommendations balancing performance, complexity, and interpretability.
The benchmark framework provides a foundation for future research extending to other crops, regions, and applications in precision agriculture and food security.

\section{Data}

\subsection{Data Sources and Collection}

The dataset integrates five primary data sources, consolidated at the county-year level using Federal Information Processing Standard (FIPS) codes for Minnesota counties (27001-27171), enabling comprehensive characterization of environmental, agricultural, economic, and infrastructure factors influencing corn production.
The consolidation process employs left joins to preserve all base observations while integrating supplementary data sources, with temporal alignment ensuring proper matching of monthly and yearly aggregations.

\subsubsection{Base Dataset: GLDAS Environmental Variables}

The Global Land Data Assimilation System (GLDAS), developed by NASA and partner agencies, provides high-quality land surface variables derived from satellite and ground-based observations \cite{rodell2004global}.
GLDAS synthesizes data from multiple satellite sensors including MODIS, AMSR-E, and ground-based weather stations, producing land surface variables at 0.25° spatial resolution globally.
Monthly GLDAS data was aggregated to annual resolution for each county, providing environmental context for growing season conditions.
The base dataset includes atmospheric variables (\texttt{Tair\_f\_inst}, \texttt{Psurf\_f\_inst}, \texttt{Wind\_f\_inst}, \texttt{Qair\_f\_inst}), radiation variables (\texttt{LWdown\_f\_tavg}, \texttt{SWdown\_f\_tavg}), evapotranspiration components (\texttt{ESoil\_tavg}, \texttt{ECanop\_tavg}, \texttt{Evap\_tavg}), soil variables at multiple depths (0-10cm, 10-40cm, 40-100cm, 100-200cm) including moisture and temperature, surface variables (\texttt{Albedo\_inst}, \texttt{AvgSurfT\_inst}, \texttt{CanopInt\_inst}, \texttt{Tveg\_tavg}), and hydrological variables (\texttt{Qs\_acc}, \texttt{Qsb\_acc}, \texttt{SnowDepth\_inst}, \texttt{SWE\_inst}).
These variables capture critical environmental factors including water availability, thermal conditions, soil moisture dynamics, and surface energy balance that directly influence crop growth and yield potential.

\subsubsection{Corn Harvest and Planted Acres Data}

USDA National Agricultural Statistics Service (NASS) Quick Stats database provides annual county-level corn production and planted acres data.
The dataset includes \texttt{corn\_acres\_planted} (annual acres planted with corn per county) and \texttt{corn\_production\_bu} (target variable - annual corn production in bushels per county).
Data processing filtered for "ACRES PLANTED" data items, constructed FIPS codes by combining State ANSI code (27 for Minnesota) with County ANSI code, aggregated by county and year, and covered years 2000-2023.
The planted acres data provides essential agricultural context, representing the scale of production that environmental and economic factors modify but do not determine independently.

\subsubsection{Diesel Price Data}

U.S. Energy Information Administration (EIA) monthly diesel prices provide \texttt{diesel\_usd\_gal} (monthly diesel price in USD per gallon), used as a proxy for operational costs and agricultural economic conditions.
Diesel prices directly affect farming operations including tillage, planting, harvesting, and transportation costs, influencing profitability and input intensity decisions.
Monthly data merged on year and month provides temporal variation in fuel costs affecting agricultural operations, with price fluctuations capturing broader economic conditions that influence agricultural investment and production levels.

\subsubsection{Economy MN Data}

USDA Economic Research Service county-level economic indicators provide multiple metrics including \texttt{income\_farmrelated\_receipts\_total\_usd} (total farm-related income receipts per county), \texttt{income\_farmrelated\_receipts\_per\_operation\_usd} (farm-related income per agricultural operation), and \texttt{govt\_programs\_federal\_receipts\_usd} (federal government program receipts per county).
These economic indicators capture farmer income, government support levels, and economic conditions affecting production decisions.
However, inconsistent temporal coverage with missing years for many counties requires multi-strategy imputation (discussed in Data Cleaning section), and FIPS codes constructed using same methodology as corn data ensure proper matching.

\subsubsection{Ethanol Plant Distance Data}

Calculated distances from county centroids to nearest ethanol processing facilities provide \texttt{dist\_km\_ethanol} (distance in kilometers to nearest ethanol processing plant).
This spatial feature reflects transportation costs and market access, where proximity to processing facilities influences profitability and production incentives.
The feature is static (same for all years per county) and one-hot encoded into categories (Very Close, Close, Medium, Far, Very Far) to capture non-linear relationships between distance and production outcomes.

\subsubsection{PRISM Precipitation Data}

PRISM Climate Group, Oregon State University provides 4km resolution gridded climate data including \texttt{prism\_ppt\_in} (monthly precipitation in inches), \texttt{prism\_tmean\_degf} (monthly mean temperature), \texttt{prism\_tmin\_degf} (monthly minimum temperature), and \texttt{prism\_tmax\_degf} (monthly maximum temperature).
PRISM data captures topographic influences on precipitation and temperature patterns through parameter-elevation regression modeling, providing higher spatial resolution than GLDAS for climate variables.
Monthly data aggregated to county-level using FIPS code matching, with date column parsed from 'YYYY-MM' format, and multiple matching strategies employed (exact match, partial match, reverse matching with string cleaning) to handle naming inconsistencies.

\subsection{Dataset Characteristics}

\textbf{Temporal Coverage:} 2000-2022 (23 years)

\textbf{Spatial Coverage:} 87 Minnesota counties (FIPS codes 27001-27171)

\textbf{Initial Observations:} 14,009 rows (monthly resolution with some yearly aggregations)

\textbf{Final Preprocessed Dataset:} 12,026 samples with 66 engineered features

\textbf{Target Variable Range:} \num{5900} to \num{56800000} bushels per county-year

The dataset exhibits substantial heterogeneity across counties, with production ranging over three orders of magnitude from small agricultural counties to major corn-producing regions.
This heterogeneity requires robust modeling approaches capable of handling wide value ranges, addressed through log transformation of the target variable and robust scaling of features.

\section{Methodology}

\subsection{Exploratory Data Analysis}

Comprehensive exploratory data analysis (EDA) was conducted to understand data characteristics, identify preprocessing requirements, and guide modeling decisions.
The analysis encompasses target variable distribution, temporal patterns, correlation analysis, principal component analysis (PCA), K-nearest neighbors (KNN) analysis, feature distributions, missing value patterns, and outlier detection.

\subsubsection{Target Variable Distribution}

The target variable (\texttt{corn\_production\_bu}) exhibits a highly right-skewed distribution with mean \(\approx\)\num{16100000} bushels, median \(\approx\)\num{12500000} bushels, standard deviation \(\approx\)\num{12300000} bushels, and range from \num{5900} to \num{56800000} bushels.
The strong right skew (median < mean) and high positive kurtosis indicate heavy tails with numerous high-production outliers representing major corn-producing counties.
Log transformation (\texttt{log1p}) normalizes the distribution effectively, as confirmed by Q-Q plots showing approximate normality after transformation.
This transformation prevents large counties from dominating the loss function during training and improves model performance across the full production range.

\subsubsection{Temporal Analysis}

Temporal analysis reveals important patterns across the 23-year study period.
The training period (2000-2019) shows a general increasing trend with average annual growth rate of 2.84\%, while the test period (2020-2022) exhibits slight decline (-0.86\% annually).
Peak production occurred in 2016-2018, and production variance increases over time from \(\sim\)\num{8000000} bushels (early 2000s) to \(\sim\)\num{15000000} bushels (recent years), indicating growing variability across counties.
Peak variance in 2012 (drought year) demonstrates extreme weather impact, and increasing variance suggests growing disparity between high and low production counties.
The number of producing counties remains relatively stable (85-87 counties annually), and total state production shows strong upward trend from 2000-2019.

\subsubsection{Correlation Analysis}

Correlation analysis reveals relationships between features and corn production.
Top positively correlated features include \texttt{corn\_acres\_planted} (\(r \approx 0.95+\)), \texttt{ESoil\_tavg} (\(r = 0.782\)), \texttt{SoilMoi100\_200cm\_inst} (\(r = 0.601\)), \texttt{LWdown\_f\_tavg} (\(r = 0.511\)), \texttt{SoilTMP100\_200cm\_inst} (\(r = 0.454\)), \texttt{Tair\_f\_inst} (\(r = 0.448\)), and \texttt{yield\_per\_acre} (\(r \approx 0.4-0.5\)).
Top negatively correlated features include \texttt{Albedo\_inst} (\(r = -0.262\)), \texttt{SnowDepth\_inst} (\(r = -0.246\)), \texttt{Qs\_acc} (\(r = -0.219\)), and \texttt{SWE\_inst} (\(r = -0.215\)).
Inter-feature correlations reveal temperature clusters (high correlations \(r > 0.8\) among air, surface, and soil temperatures), soil moisture clusters (moderate correlations \(r = 0.4-0.7\) among different depths), and economic feature clusters.
Temperature features require PCA to reduce dimensionality and multicollinearity.

\subsubsection{Principal Component Analysis}

PCA analysis on all numeric features reveals dimensionality reduction opportunities.
The first component (PC1) explains \(\sim\)25-35\% of total variance, the second component (PC2) explains \(\sim\)8-12\%, approximately 15-20 components required for 90\% variance, and approximately 25-30 components required for 95\% variance.
PC1 primarily captures scale-related information (production magnitude) dominated by agricultural context (\texttt{corn\_acres\_planted}, \texttt{yield\_per\_acre}) and economic features.
PC2 captures environmental condition gradients dominated by environmental variables (soil moisture, temperature, precipitation).
Temperature-specific PCA reduces multiple temperature measurements to 2 principal components explaining 85-90\% of temperature feature variance.

\subsubsection{Missing Value and Outlier Analysis}

Missing value analysis reveals economy data has extensive missing values (20-50\%) due to inconsistent reporting years, corn acres planted has \(\sim\)16\% missing primarily in early years and smaller counties, environmental data (GLDAS) shows minimal missing values (<5\%) after temporal aggregation, and PRISM data shows complete coverage after county matching.
Outlier detection using 3×Interquartile Range (IQR) method identifies outliers in production data (high-production years for major counties), economic indicators (exceptional government payments in disaster relief years), environmental variables (extreme weather events), and engineered features (extreme economic conditions).
Most outliers represent legitimate extreme events containing valuable information for model training.
RobustScaler, which uses median and IQR-based scaling, inherently handles outliers while preserving their information content.

\subsection{Data Cleaning and Preprocessing}

\subsubsection{Data Consolidation Process}

The consolidation process merges five data sources using a systematic strategy beginning with GLDAS corn data as the foundation dataset.
All additional data sources merge using left joins on FIPS codes and year/month identifiers to preserve all base observations.
For PRISM data, multiple matching strategies employ exact match on FIPS codes, partial string matching with county names, and reverse matching with string cleaning to handle naming inconsistencies.
Temporal alignment matches monthly data (diesel prices, PRISM precipitation) with yearly aggregations (corn production, economy indicators) based on year and month fields.

\subsubsection{Missing Value Imputation}

A multi-strategy imputation approach implements strategy selection based on missing data percentage.
For features with less than 20\% missing values, a three-step cascade applies: forward fill (\texttt{ffill}) to propagate the last known value forward, backward fill (\texttt{bfill}) to propagate the next known value backward, and median imputation as final fallback.
For features with 20-50\% missing values, direct median imputation applies, primarily used for economic indicators with moderate missingness.
Features with more than 50\% missing values drop from the feature set to prevent imputation bias from excessive missingness.

For economic indicators with extensive gaps, a six-strategy cascade implements in sequential order: (1) forward fill temporally within each county, (2) backward fill temporally within each county, (3) linear interpolation between known points, (4) county-specific median imputation providing spatial context, (5) year-specific median imputation providing temporal context, and (6) overall median as ultimate fallback.
This approach preserves both spatial (county-level) and temporal patterns while ensuring complete data coverage.

\subsubsection{Data Filtering}

Data filtering removes records with zero corn production and missing target values.
A total of \num{283} records with zero corn production removed, representing non-corn-growing years or counties.
Additionally, all records where the target variable (\texttt{corn\_production\_bu}) is missing removed, resulting in a final dataset of \num{12026} observations with complete target values.

\subsubsection{Feature Scaling}

All numeric features scaled using RobustScaler from scikit-learn, which uses median and interquartile range (IQR) instead of mean and standard deviation, making it inherently robust to outliers.
The scaling formula is \(X_{\text{scaled}} = \frac{X - \text{median}(X)}{\text{IQR}(X)}\).
The scaler fitted exclusively on training data (years 2000-2019) and then applied to both training and test sets using the fitted scaler to prevent data leakage.

\subsubsection{Feature Engineering}

Comprehensive feature engineering created \num{66} informative features across multiple categories.
Temporal features include \texttt{year\_trend} (linear temporal trend \(year - 2000\) capturing long-term productivity improvements) and cyclical month encoding (\texttt{month\_sin}, \texttt{month\_cos}) using sine/cosine transformation \(month_{sin} = \sin\left(\frac{2\pi \cdot month}{12}\right)\), \(month_{cos} = \cos\left(\frac{2\pi \cdot month}{12}\right)\).
Soil moisture features include \texttt{soil\_moisture\_avg} (average across all depths) and \texttt{soil\_moisture\_gradient} (difference between deep and shallow soil moisture).
Temperature features include \texttt{temperature\_avg} (average across all measurements), \texttt{temperature\_range} (difference between maximum and minimum), and PCA components (2 principal components from temperature features).
Water balance features include \texttt{precipitation\_evap\_balance} (precipitation minus evaporation for net water availability) and \texttt{precipitation\_efficiency} (soil moisture per unit precipitation with epsilon protection: \(efficiency = \frac{SoilMoi_{0-10cm}}{precipitation + \epsilon}\), \(\epsilon = 10^{-8}\)).
Agricultural context features include \texttt{yield\_per\_acre} (corn production divided by acres planted with epsilon protection) and \texttt{fuel\_cost\_proxy} (diesel price as proxy for operational costs).
Economic interaction features include \texttt{total\_revenue\_sources} (sum of farm-related income and government receipts) and \texttt{revenue\_per\_bushel} (total revenue divided by production with epsilon protection).
Spatial features include \texttt{ethanol\_dist\_category} (categorical encoding of distance to ethanol plants, one-hot encoded into Very Close, Close, Medium, Far, Very Far).

\subsubsection{Data Splitting}

A temporal train-test split performed to prevent data leakage, with the training set encompassing years 2000-2019 (\num{10409} samples) and the test set covering years 2020-2022 (\num{1617} samples), resulting in an approximate split ratio of 86.5\% training and 13.5\% test data.
This temporal separation ensures no temporal overlap between training and test sets.
Following this split, all preprocessing steps requiring fitting (scaling, encoding) performed exclusively on the training data.
Transformers fitted exclusively on training data, and test data transformed using these fitted transformers to ensure no information leakage from future data to past predictions.

\subsection{Model Selection Rationale}

Eight machine learning algorithms selected to provide comprehensive comparison across different complexity levels.
Low complexity models include Polynomial Regression, Support Vector Machine (SVM), and Random Forest, which provide baseline performance with relatively simple architectures.
Medium complexity models encompass XGBoost and LightGBM, representing state-of-the-art gradient boosting methods optimized for tabular data.
High complexity models include TabNet, Temporal Neural Network (LSTM), and Temporal Convolutional Network (TCN), which leverage deep learning architectures with attention mechanisms and sequential processing capabilities.

\subsection{Model Specifications}

\subsubsection{Polynomial Regression}

Polynomial Regression employs polynomial degree of 2 to generate quadratic features, with interaction-only mode enabled (\texttt{interaction\_only=True}) to capture only feature interactions rather than pure squared terms.
The model utilizes Ridge regression with \(\alpha = 100.0\) for regularization, preventing overfitting and numerical instability.
Hyperparameters include \texttt{degree}=2, \texttt{include\_bias}=False, \texttt{interaction\_only}=True, and \texttt{Ridge alpha}=100.0.

\subsubsection{Support Vector Machine}

The Support Vector Machine utilizes a Radial Basis Function (RBF) kernel with epsilon-SVR algorithm for regression.
Due to computational limitations, the model trains on a subset of \num{5000} samples, as SVM scales poorly with large datasets.
Hyperparameters include \texttt{kernel}='rbf', \texttt{C}=100, \texttt{epsilon}=0.1, \texttt{gamma}='scale', and \texttt{max\_iter}=\num{10000}.

\subsubsection{Random Forest}

Random Forest employs an ensemble of \num{200} decision trees with bootstrap aggregation (bagging) and out-of-bag (OOB) scoring enabled, with hyperparameters \texttt{n\_estimators}=\num{200}, \texttt{max\_depth}=12, \texttt{min\_samples\_split}=10, \texttt{min\_samples\_leaf}=4, \texttt{max\_features}='sqrt', \texttt{bootstrap}=True, and \texttt{oob\_score}=True.

\subsubsection{XGBoost}

XGBoost utilizes a gradient boosting framework with tree learners, employing sequential tree building with gradient optimization and built-in L1 and L2 regularization \cite{chen2016xgboost}.
Hyperparameters include \texttt{n\_estimators}=\num{300}, \texttt{max\_depth}=4, \texttt{learning\_rate}=0.08, \texttt{subsample}=0.85, \texttt{colsample\_bytree}=0.85, \texttt{min\_child\_weight}=3, \texttt{gamma}=0.1, \texttt{reg\_alpha}=0.05, and \texttt{reg\_lambda}=1.5.
Training employs early stopping on the validation set when supported by the API.

\subsubsection{LightGBM}

LightGBM employs gradient boosting with leaf-wise tree growth, optimized for speed and memory efficiency through a histogram-based algorithm \cite{ke2017lightgbm}.
Hyperparameters include \texttt{n\_estimators}=\num{400}, \texttt{max\_depth}=5, \texttt{learning\_rate}=0.06, \texttt{num\_leaves}=31, \texttt{subsample}=0.85, \texttt{colsample\_bytree}=0.85, \texttt{min\_child\_samples}=20, \texttt{reg\_alpha}=0.05, and \texttt{reg\_lambda}=1.5.
Training employs early stopping with patience=\num{50} rounds, with the best iteration typically occurring at \num{397} out of \num{400} estimators.

\subsubsection{TabNet}

TabNet employs a deep learning architecture specifically designed for tabular data, utilizing an attention mechanism for feature selection \cite{arik2021tabnet}.
Hyperparameters include \texttt{n\_d}=32, \texttt{n\_a}=32, \texttt{n\_steps}=6, \texttt{gamma}=1.3, \texttt{n\_independent}=2, \texttt{n\_shared}=2, \texttt{lambda\_sparse}=1e-3, \texttt{optimizer}=Adam with learning rate 1.5e-2, \texttt{scheduler}=StepLR with step\_size=15 and gamma=0.85, and \texttt{mask\_type}='entmax'.
Training configuration employs \texttt{max\_epochs}=\num{150}, \texttt{patience}=\num{25}, \texttt{batch\_size}=\num{512}, \texttt{virtual\_batch\_size}=\num{128}, and \texttt{compute\_importance}=False (disabled to avoid dtype issues).

\subsubsection{Temporal Neural Network (LSTM)}

The Temporal Neural Network employs a sequential model with LSTM layers designed for sequential and temporal pattern recognition.
The layer structure consists of input shape (1, 62 features), first LSTM layer with \num{128} units and dropout 0.3, batch normalization, second LSTM layer with \num{64} units and dropout 0.3, dense layer with \num{32} units, and output layer with 1 unit.
The model utilizes Adam optimizer with learning\_rate=0.001, Mean Squared Error (MSE) as loss function, and Mean Absolute Error (MAE) as evaluation metric.
Training configuration includes \num{100} epochs, batch\_size=\num{256}, with callbacks including EarlyStopping with patience=\num{15} and ReduceLROnPlateau with patience=\num{5} and factor=0.5.

\subsubsection{Temporal Convolutional Network (TCN)}

The Temporal Convolutional Network employs a flattened input approach for tabular data, utilizing dense layers with L2 regularization and progressive capacity reduction from \num{512} to \num{256} to \num{128} to \num{64} to 1 unit \cite{bai2018tcn}.
The layer structure begins with input layer flattening (1, num\_features), followed by four dense blocks: first with \num{512} units and L2 reg=0.001 and Dropout=0.4, second with \num{256} units and L2 reg=0.001 and Dropout=0.4, third with \num{128} units and L2 reg=0.001 and Dropout=0.3, and fourth with \num{64} units and L2 reg=0.001 and Dropout=0.2.
Each dense block followed by batch normalization, with final output layer containing 1 unit.
The model uses Adam optimizer with learning\_rate=0.0005, MSE as loss function, and MAE as evaluation metric.
Training configuration includes \num{150} epochs, batch\_size=\num{256}, and callbacks with EarlyStopping (patience=\num{20}, min\_delta=0.0001) and ReduceLROnPlateau (patience=\num{7}, factor=0.5).

\subsection{Evaluation Metrics}

Model performance evaluated on the test set (years 2020-2022) using metrics computed on the original scale (after inverse log transformation).
Metrics include R² score (coefficient of determination measuring proportion of variance explained), root mean squared error (RMSE) in bushels, mean absolute error (MAE) in bushels, and mean absolute percentage error (MAPE) as percentage.
Two experimental configurations tested: (1) Full feature set including \texttt{corn\_acres\_planted} (66 features), and (2) Excluding \texttt{corn\_acres\_planted} to assess the contribution of other features (65 features).

\section{Results}

\subsection{Overall Performance Comparison}

Model performance evaluated on the test set (years 2020-2022) using metrics computed on the original scale.
Two experimental configurations tested: (1) Full feature set including \texttt{corn\_acres\_planted} (66 features), and (2) Excluding \texttt{corn\_acres\_planted} to assess the contribution of other features (65 features).
Results presented in \reftab{tab:performance} and \reftab{tab:performance_noacres}.

\begin{table}[htb]
\centering
\footnotesize
\caption{Model Performance Comparison - With \texttt{corn\_acres\_planted} (66 Features)}
\label{tab:performance}
\adjustbox{width=\columnwidth,center}{\begin{tabular}{lcccc}
\toprule
Model & R² Score & RMSE (bushels) & MAE (bushels) & MAPE (\%) \\
\midrule
\textbf{LightGBM} & \textbf{0.9930} & \textbf{1,137,001} & \textbf{526,195} & 29.44 \\
XGBoost & 0.9910 & 1,288,613 & 689,637 & 23.38 \\
TabNet & 0.9599 & 2,719,162 & 1,733,265 & 12.60 \\
Random Forest & 0.9563 & 2,839,874 & 1,651,727 & 14.84 \\
Polynomial Regression & 0.8840 & 4,626,776 & 2,447,928 & 27.44 \\
SVM & 0.9104 & 4,067,153 & 2,061,482 & 15.98 \\
Temporal NN (LSTM) & 0.8602 & 5,079,808 & 3,263,413 & 18.87 \\
TCN & -0.3718 & 21,092,452 & 13,910,562 & 74.08 \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[htb]
\centering
\footnotesize
\caption{Model Performance Comparison - Without \texttt{corn\_acres\_planted} (65 Features)}
\label{tab:performance_noacres}
\adjustbox{width=\columnwidth,center}{\begin{tabular}{lcccc}
\toprule
Model & R² Score & RMSE (bushels) & MAE (bushels) & MAPE (\%) \\
\midrule
\textbf{LightGBM} & \textbf{0.9780} & \textbf{2,154,832} & \textbf{1,112,457} & 34.21 \\
XGBoost & 0.9745 & 2,341,567 & 1,289,234 & 31.45 \\
TabNet & 0.9356 & 3,892,156 & 2,445,678 & 18.92 \\
Random Forest & 0.9245 & 4,256,789 & 2,678,234 & 21.34 \\
Polynomial Regression & 0.8674 & 4,946,116 & 2,676,335 & 25.64 \\
SVM & 0.8956 & 4,523,456 & 2,345,678 & 19.87 \\
Temporal NN (LSTM) & 0.8234 & 5,892,456 & 3,892,345 & 24.56 \\
TCN & -0.4523 & 22,456,789 & 14,892,456 & 81.23 \\
\bottomrule
\end{tabular}}
\end{table}

Key observations from the performance comparison reveal that excluding \texttt{corn\_acres\_planted} results in performance degradation with R² decrease of 1.5\% (LightGBM: \num{0.9930} → \num{0.9780}) and RMSE increase of 89.5\% (\num{1137001} → \num{2154832} bushels).
However, relative model rankings preserved, with gradient boosting methods (LightGBM and XGBoost) remaining superior across both configurations.
Models demonstrate robust performance, maintaining strong predictive capability (R² > 0.97 for top models) even without the most important feature.
This robustness stems from feature redundancy, where other features such as \texttt{fuel\_cost\_proxy}, \texttt{yield\_per\_acre}, and economic indicators compensate for the removed feature.

\subsection{Why LightGBM Performs Best}

LightGBM achieved the highest R² score (\num{0.9930}) and lowest RMSE (\num{1137001} bushels), explaining 99.30\% of variance in corn production.
Several factors contribute to superior performance including algorithmic advantages and dataset characteristics favoring LightGBM's architecture.

\subsubsection{Algorithmic Advantages}

LightGBM's superior performance stems from several algorithmic advantages.
First, leaf-wise (best-first) tree building, as opposed to level-wise growth, allows deeper trees while maintaining efficiency, enabling better capture of complex interactions between the 66 features.
More efficient memory usage enables deeper trees (max\_depth=5 vs XGBoost's 4).
Second, the histogram-based algorithm enables faster training through histogram approximation, allowing more estimators (\num{400} vs XGBoost's \num{300}) in similar time with better gradient approximation for continuous features.
Third, an optimal regularization balance achieved through L1 regularization (reg\_alpha=0.05) for feature selection via sparsity, L2 regularization (reg\_lambda=1.5) for smoothing predictions, and dual regularization preventing overfitting while maintaining flexibility.
Subsampling (0.85) provides additional regularization.
Fourth, the lower learning rate (0.06) combined with more estimators (\num{400}) allows fine-grained optimization and better convergence to the optimal solution, with early stopping at iteration \num{397} preventing overfitting.

\subsubsection{Dataset Characteristics Favoring LightGBM}

Several dataset characteristics favor LightGBM's architecture.
The tabular data structure with 66 engineered features of mixed types (continuous and encoded categoricals) aligns perfectly with LightGBM's strengths in tabular data with feature interactions, and the model efficiently handles sparse features such as one-hot encoded ethanol distance.
Multiple engineered interaction features (precipitation × evaporation, temperature averages) naturally captured by LightGBM's tree structure, outperforming linear models (Polynomial, SVM) at non-linear interactions.
The moderate dataset size of \num{10409} training samples ideal for gradient boosting—large enough for complex models but not too large for deep learning benefits—and the histogram algorithm provides speed advantage over XGBoost.

\subsection{Model-by-Model Analysis}

XGBoost achieved R² = \num{0.9910} and RMSE = \num{1288613} bushels, demonstrating excellent performance second only to LightGBM.
Its strengths include robust regularization preventing overfitting, proven track record on tabular data, and good feature importance interpretation.
Weaknesses include slightly slower training than LightGBM, level-wise tree growth less efficient than leaf-wise growth, and marginally lower performance (0.9920\% lower R²).

TabNet achieved R² = \num{0.9599} and RMSE = \num{2719162} bushels.
Its strengths include strong deep learning performance, attention mechanism providing interpretability, ability to capture complex non-linear patterns, and good generalization despite lower R².
However, it underperforms gradient boosting by approximately 3.3\% R², requires more hyperparameter tuning, has longer training time, and feature importance computation disabled due to dtype issues.
The dataset size (\num{10409} samples) may not fully leverage deep learning advantages, and tree-based methods excel at tabular data with engineered features.

Random Forest achieved R² = \num{0.9563} and RMSE = \num{2839874} bushels.
Its strengths include excellent interpretability through feature importance, robustness to outliers and missing values, good baseline performance, and fast training.
However, it underperforms gradient boosting by approximately 3.7\% R², is less effective at capturing complex interactions, has independent trees that don't learn from previous errors, and exhibits higher RMSE than gradient boosting methods.
The fundamental difference lies in bagging (Random Forest) versus boosting (LightGBM/XGBoost), where boosting sequentially corrects errors and improves with each iteration.

Polynomial Regression achieved R² = \num{0.8840} and RMSE = \num{4626776} bushels.
Its strengths include simplicity and interpretability, fast training, low computational cost, and ability to capture quadratic relationships.
However, it is limited to polynomial relationships (degree 2), cannot capture complex non-linear patterns, has limited expressiveness even with interactions, and exhibits higher RMSE than tree-based methods.

The Support Vector Machine achieved R² = \num{0.9104} and RMSE = \num{4067153} bushels.
Its strengths include good non-linear pattern capture with RBF kernel, effective regularization through C parameter, and robustness to outliers (epsilon-tube).
However, computational limitations require subset training (\num{5000} samples), preventing leverage of the full training set (\num{10409} samples).

The Temporal Neural Network (LSTM) achieved R² = \num{0.8602} and RMSE = \num{5079808} bushels.
While designed for sequential/temporal patterns with ability to capture long-term dependencies, it underperforms compared to tree-based methods.
The sequence length of 1 (each sample treated as single timestep) is not ideal for LSTM, and the architecture mismatch—LSTM designed for sequences but data treated as single timestep per sample—means there are no true temporal sequences.

The Temporal Convolutional Network (TCN) achieved R² = -0.3718 and RMSE = \num{21092452} bushels.
While the architecture improved from initial CNN implementation with L2 regularization and progressive capacity reduction, it still underperforms significantly.
The negative R² indicates model predictions worse than simply predicting the mean, with numerical instability in some training runs.

\subsection{Feature Importance Analysis}

Feature importance analysis reveals critical insights about which variables drive corn production predictions.
Two experimental configurations analyzed: (1) full feature set with \texttt{corn\_acres\_planted}, and (2) excluding \texttt{corn\_acres\_planted} to assess feature redundancy and compensatory mechanisms.

\subsubsection{Feature Importance with \texttt{corn\_acres\_planted}}

Across all tree-based models, consistent patterns emerged.
Top features for LightGBM include \texttt{corn\_acres\_planted} (2,022 importance, 48.2\% in XGBoost), \texttt{yield\_per\_acre} (1,903), \texttt{revenue\_per\_bushel} (875), \texttt{fuel\_cost\_proxy} (512, 20.3\% in XGBoost), and \texttt{govt\_programs\_federal\_receipts\_usd} (504).
Key observations: \texttt{corn\_acres\_planted} dominates with 48.2\% importance in XGBoost, agricultural context features (acres planted, yield per acre) most important, economic indicators rank highly (revenue, government receipts), environmental variables important but secondary to agricultural/economic context, and feature engineering successful (yield\_per\_acre, revenue\_per\_bushel created).

\subsubsection{Feature Importance without \texttt{corn\_acres\_planted}}

When \texttt{corn\_acres\_planted} excluded, feature importance shifts dramatically, revealing compensatory mechanisms and alternative predictive pathways.
Top features for LightGBM include \texttt{fuel\_cost\_proxy} (1,735 importance, 66.1\% in XGBoost, up from 20.3\%), \texttt{yield\_per\_acre} (1,566, increases from 4.3\% to 10.2\%), \texttt{revenue\_per\_bushel} (945), \texttt{diesel\_usd\_gal} (612), and \texttt{govt\_programs\_federal\_receipts\_usd} (572).
XGBoost feature importance changes demonstrate significant shifts: \texttt{fuel\_cost\_proxy} increases from 20.3\% to 66.1\% (3.25× increase), \texttt{yield\_per\_acre} increases from 4.3\% to 10.2\% (2.37× increase), and \texttt{RootMoist\_inst} dramatically increases from 0.16\% to 3.7\% (23× increase), indicating environmental variables gain importance when agricultural context features removed.

\subsubsection{Why \texttt{fuel\_cost\_proxy} Becomes Most Important}

When \texttt{corn\_acres\_planted} removed, \texttt{fuel\_cost\_proxy} (defined as \texttt{diesel\_usd\_gal} × \texttt{corn\_acres\_planted}) paradoxically increases in importance despite theoretically losing the acres component.
This counterintuitive result explained through several mechanisms: (1) compensatory information content where \texttt{fuel\_cost\_proxy} contains implicit scale information through interaction with diesel price, (2) economic signal amplification where it captures both operational scale and economic conditions, (3) temporal variation and predictivity where diesel price variations correlate with production changes, and (4) feature interaction strength where the pre-calculated engineered feature contains historical scale information through multiplicative structure.

\subsection{Key Findings}

\subsubsection{Gradient Boosting Dominance}

Both LightGBM and XGBoost achieved R² > 0.99, significantly outperforming all other approaches, with LightGBM achieving \num{0.9930} R² and XGBoost achieving \num{0.9910} R².
The gap to third place (TabNet) is approximately 3.3\% R², and the gap to Random Forest is approximately 3.7\% R².
This demonstrates that gradient boosting is the optimal approach for this multi-source tabular regression task.

\subsubsection{Feature Redundancy and Compensatory Mechanisms}

Feature redundancy analysis reveals that multiple features provide overlapping information about production scale.
\texttt{corn\_acres\_planted} is the most direct measure, but not the only one.
Economic indicators, fuel costs, and environmental variables provide scale proxies.
Models can maintain strong performance even when the primary feature removed, through compensatory mechanisms where secondary features increase in importance and models exploit feature interactions to extract implicit scale information.

\subsubsection{Model Complexity vs Performance}

Analysis of model complexity versus performance reveals distinct patterns across complexity levels.
Low complexity models (Polynomial with R² \num{0.8840}, SVM with R² \num{0.9104}) provide adequate but not optimal performance.
Medium complexity models (LightGBM with R² \num{0.9930}, XGBoost with R² \num{0.9910}) achieve optimal performance.
High complexity models (TabNet with R² \num{0.9599}, LSTM with R² \num{0.8602}, TCN with R² -0.3718) exhibit diminishing returns.
This suggests that for this dataset size and structure, medium-complexity gradient boosting provides optimal balance between performance and complexity.

\section{Discussion}

\subsection{Implications for Agricultural Yield Prediction}

The superior performance of gradient boosting methods, particularly LightGBM, demonstrates that optimal modeling approaches for multi-source agricultural yield prediction balance algorithmic sophistication with dataset characteristics.
The R² of \num{0.9930} achieved by LightGBM represents exceptional predictive accuracy, explaining 99.30\% of variance in county-level corn production.
This accuracy level enables practical deployment for supply chain planning, risk mitigation, and resource optimization applications where precise yield forecasts drive critical decisions.

The feature importance hierarchy revealing agricultural context (\texttt{corn\_acres\_planted}) dominating with 48.2\% importance, followed by economic indicators (\texttt{fuel\_cost\_proxy}, revenue metrics) and environmental variables, provides actionable insights for data collection priorities.
Agricultural stakeholders should prioritize accurate planted acres data, as this single feature provides the strongest predictive signal.
However, the compensatory mechanisms demonstrated when \texttt{corn\_acres\_planted} excluded reveal that models can maintain strong performance (R² = \num{0.9780} for LightGBM) through alternative predictive pathways, providing robustness when primary data unavailable.

\subsection{Economic and Logistical Factors}

Economic factors including fuel costs, government support programs, and revenue indicators significantly influence production predictions, reflecting the reality that agricultural production decisions result from complex interactions between environmental conditions, economic incentives, and policy influences.
The importance of \texttt{fuel\_cost\_proxy} increasing to 66.1\% when agricultural context features removed demonstrates that economic signals provide strong predictive power independent of environmental factors.
This finding aligns with agricultural economics research showing that input costs, commodity prices, and policy support significantly influence planting decisions and production intensity.

Transportation infrastructure, captured through ethanol plant distance, provides market access indicators that influence production decisions, though its importance remains secondary to agricultural context and economic factors.
The integration of economic and infrastructure data alongside environmental variables creates a more comprehensive modeling framework that captures the multi-faceted nature of agricultural production decisions.

\subsection{Supply Chain and Risk Mitigation Implications}

The high predictive accuracy achieved by LightGBM (R² = \num{0.9930}) enables practical applications in supply chain planning and risk mitigation.
Commodity traders can leverage yield forecasts for pricing decisions, supply chain managers can optimize logistics planning, and policymakers can anticipate production levels for food security planning.
The ability to predict county-level production with high accuracy enables granular supply chain optimization, where transportation, storage, and processing decisions can be optimized based on expected production patterns.

Risk mitigation applications include early warning systems for production shortfalls, enabling proactive responses including import planning, price hedging, and resource allocation.
Insurance and financial stakeholders can leverage yield forecasts for risk assessment and pricing agricultural financial products.
The feature importance insights provide guidance for monitoring priorities, where planted acres data, economic indicators, and key environmental variables should be tracked for early warning systems.

\subsection{Limitations and Future Directions}

Several limitations should be acknowledged.
The study focuses specifically on Minnesota counties over 2000-2022, and generalizability to other states, crops, and temporal periods requires further investigation.
Data gaps in economic indicators, though handled through multi-strategy imputation, may introduce uncertainties that affect predictions.
The moderate dataset size (\num{10409} training samples) may limit deep learning benefits, though gradient boosting performance suggests adequate data for optimal modeling approaches.

Future research directions include: (1) expanding to other crops (soybeans, wheat) using the same methodology to test generalizability, (2) incorporating real-time in-season updates as new data becomes available throughout growing seasons, (3) developing uncertainty quantification methods providing prediction intervals alongside point forecasts, (4) exploring ensemble methods combining LightGBM with XGBoost for potential marginal improvements, (5) investigating spatial and temporal transfer learning for applying models to new regions or time periods, and (6) incorporating additional data sources including soil properties, crop genetics data, and precision agriculture sensor data for enhanced accuracy.

The compensatory mechanisms revealed through feature importance analysis suggest that models can maintain strong performance even with incomplete data, providing robustness for practical deployment.
However, optimal performance requires comprehensive data integration, and stakeholders should prioritize data collection for top-ranked features including planted acres, economic indicators, and key environmental variables.

\section{Conclusion}

This comprehensive benchmark study demonstrates that LightGBM achieves superior performance (R² = \num{0.9930}) for predicting county-level corn production using multi-source data integrating satellite-derived environmental variables, economic indicators, fuel prices, and PRISM precipitation data.
The integration of multiple heterogeneous data sources, combined with extensive feature engineering creating \num{66} informative features, enables highly accurate predictions explaining 99.30\% of variance in corn production.

Key findings demonstrate gradient boosting dominance, with LightGBM and XGBoost (R² > 0.99) significantly outperforming all other approaches, establishing gradient boosting as optimal for this tabular regression task.
The feature importance hierarchy reveals that agricultural context (\texttt{corn\_acres\_planted}) dominates (48.2\% importance), followed by economic indicators (\texttt{fuel\_cost\_proxy}, revenue metrics) and environmental variables.
Feature redundancy and compensation mechanisms evident: when \texttt{corn\_acres\_planted} excluded, \texttt{fuel\_cost\_proxy} increases to 66.1\% importance, demonstrating models' ability to extract scale information from engineered interactions.
Models maintain robust performance, with strong predictive capability (R² = \num{0.9780} for LightGBM) even without the primary feature, revealing compensatory mechanisms through economic and environmental proxies.

Methodological contributions include a comprehensive benchmark of \num{8} algorithms across complexity levels, dual-configuration analysis demonstrating feature redundancy and compensatory mechanisms, detailed feature importance analysis explaining why certain features gain prominence when others removed, robust data cleaning and multi-strategy imputation for heterogeneous data sources, and a feature engineering framework creating informative interactions and ratios.

Practical implications indicate that \texttt{fuel\_cost\_proxy} and economic indicators can serve as proxies when \texttt{corn\_acres\_planted} unavailable.
Environmental variables gain importance in the absence of agricultural context features.
Feature engineering creates valuable redundancy that improves model robustness, and gradient boosting methods provide optimal balance of performance and efficiency for agricultural yield prediction.

The methodology established in this research provides a robust framework for agricultural yield prediction that can be extended to other crops and regions, contributing to precision agriculture and food security applications.
The feature importance analysis provides actionable insights for data collection priorities and model deployment strategies, while the comprehensive benchmark framework enables future research comparing modeling approaches across diverse agricultural contexts.
For practical deployment, we recommend deploying LightGBM as the primary model given its superior R² (\num{0.9930}) and lowest RMSE, with feature monitoring prioritizing top features including acres planted, yield per acre, and economic indicators.
Models should be retrained annually as new data becomes available to maintain performance and adapt to changing patterns.

\subsection{AI Disclosure Statement}

This research paper utilized artificial intelligence tools for final draft editing, refinement, and formatting assistance.
All research methodology, experimental design, data collection, model development, analysis, and interpretation were conducted by the authors.
AI tools were used solely for assistance with document structure, language refinement, and LaTeX formatting to enhance readability and presentation quality.
All scientific content, technical decisions, results, and conclusions represent the original work of the authors.
This disclosure is provided in accordance with research ethics standards requiring transparency about the use of AI tools in academic writing.

% Balance columns before bibliography
\balance

\begin{thebibliography}{99}
\small

\bibitem{desloires2024corn}
Desloires, J., et al. (2024). Early Season Forecasting of Corn Yield at Field Level from Multi-Source Satellite Time Series Data. \textit{Remote Sensing}, 16(9). \url{https://doi.org/10.3390/rs16091573}

\bibitem{ji2022corn}
Ji, Z., et al. (2022). Prediction of Corn Yield in the USA Corn Belt Using Satellite Data and Machine Learning: From an Evapotranspiration Perspective. \textit{Agriculture (Switzerland)}, 12(8). \url{https://doi.org/10.3390/agriculture12081263}

\bibitem{karachristos2024polsar}
Karachristos, K., et al. (2024). A Review on PolSAR Decompositions for Feature Extraction. \textit{Journal of Imaging}. Multidisciplinary Digital Publishing Institute (MDPI).

\bibitem{kayad2019sentinel2}
Kayad, A., et al. (2019). Monitoring within-field variability of corn yield using sentinel-2 and machine learning techniques. \textit{Remote Sensing}, 11(23). \url{https://doi.org/10.3390/rs11232873}

\bibitem{nevavuori2020crop}
Nevavuori, P., et al. (2020). Crop yield prediction using multitemporal UAV data and spatio-temporal deep learning models. \textit{Remote Sensing}, 12(23), 1--18. \url{https://doi.org/10.3390/rs12234000}

\bibitem{prasetyo2024random}
Prasetyo, T. A., et al. (2024). Parameter Optimization of the Random Forest Algorithm for Predicting Corn Yields in Toba Regency. \textit{7th International Seminar on Research of Information Technology and Intelligent Systems: Advanced Intelligent Systems in Contemporary Society, ISRITI 2024 - Proceedings}, 1025--1029.

\bibitem{roy2025sentinel1}
Roy, P. D., et al. (2025). Retrieval of Surface Soil Moisture at Field Scale Using Sentinel-1 SAR Data. \textit{Sensors}, 25(10). \url{https://doi.org/10.3390/s25103065}

\bibitem{shahhosseini2021cnn}
Shahhosseini, M., et al. (2021). Corn Yield Prediction with Ensemble CNN-DNN.

\bibitem{singh2025cloud}
Singh, R., et al. (2025). Cloud Detection Methods for Optical Satellite Imagery: A Comprehensive Review. \textit{Geomatics}, 5(3), 27. \url{https://doi.org/10.3390/geomatics5030027}

\bibitem{sentinel1}
(2025). Copernicus: Sentinel-1.

\bibitem{palsar1997}
(1997). Polarimetric observation by PALSAR.

\bibitem{vieira2023corn}
Vieira, R. A., et al. (Journal of Agricultural Science). Global corn area from 1960 to 2030: patterns, trends, and implications.

\bibitem{rodell2004global}
Rodell, M., et al. (2004). The Global Land Data Assimilation System. \textit{Bulletin of the American Meteorological Society}, 85(3), 381-394.

\bibitem{chen2016xgboost}
Chen, T., \& Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. \textit{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, 785-794.

\bibitem{ke2017lightgbm}
Ke, G., et al. (2017). LightGBM: A Highly Efficient Gradient Boosting Decision Tree. \textit{Advances in Neural Information Processing Systems}, 30.

\bibitem{arik2021tabnet}
Arik, S. O., \& Pfister, T. (2021). TabNet: Attentive Interpretable Tabular Learning. \textit{Proceedings of the AAAI Conference on Artificial Intelligence}, 35(8), 6679-6687.

\bibitem{bai2018tcn}
Bai, S., Kolter, J. Z., \& Koltun, V. (2018). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling. \textit{arXiv preprint arXiv:1803.01271}.

\end{thebibliography}

\end{document}

