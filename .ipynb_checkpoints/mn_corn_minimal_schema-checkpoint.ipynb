{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a24b5d5",
   "metadata": {},
   "source": [
    "\n",
    "# Minnesota Corn â€” Minimal Schema Builder (May/Jul/Aug horizons)\n",
    "\n",
    "This notebook downloads and assembles external datasets into the **minimal schema** we discussed for county-level corn modeling in Minnesota.\n",
    "\n",
    "**What you get**\n",
    "- County-month tables keyed by `fips, date, horizon_tag` with PRISM, GLDAS, NDVI, drought, ONI, soils, CDL corn fraction, basis/ethanol/diesel proxies, and NASS targets (yield, harvested acres).  \n",
    "- Helper functions to **download, cache, and aggregate** to counties (area-weighted) and compute the engineered features (e.g., **hot-day counts**, **precipâˆ’ET**, **VPD proxy**, NDVI peak and anomaly, **USDM D2+** time-in-class).\n",
    "\n",
    "> âš ï¸ **Requirements**: Internet access, and the following Python packages:  \n",
    "`pip install geopandas pandas numpy xarray rioxarray rasterio rasterstats requests tqdm pyproj shapely fiona py7zr`  \n",
    "Optional for speed: `pip install aiohttp`\n",
    "\n",
    "> ðŸ” **Credentials you must provide** (set in the next cell):  \n",
    "- **NASA Earthdata** username/password (for GLDAS) â€” used via `.netrc` cookie login.  \n",
    "- **USDA NASS QuickStats API key** (for county targets).  \n",
    "- **AppEEARS token** (optional; for MODIS NDVI); we also provide a Sentinel-2 fallback via STAC.\n",
    "\n",
    "**Output**\n",
    "- A single, tidy CSV: `/mnt/data/mn_minimal_schema.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- USER CONFIG ---\n",
    "START_YEAR = 2000\n",
    "END_YEAR   = 2024\n",
    "\n",
    "# Horizons to compute (use last day of month as cutoff)\n",
    "HORIZONS = {\n",
    "    \"MAY\": {\"month\": 5,  \"label\": \"MAY\"},\n",
    "    \"JUL\": {\"month\": 7,  \"label\": \"JUL\"},\n",
    "    \"AUG\": {\"month\": 8,  \"label\": \"AUG\"},\n",
    "}\n",
    "\n",
    "# Paths (cache)\n",
    "DATA_DIR = \"data_cache\"\n",
    "PRISM_DIR = f\"{DATA_DIR}/prism\"\n",
    "GLDAS_DIR = f\"{DATA_DIR}/gldas\"\n",
    "USDM_DIR  = f\"{DATA_DIR}/usdm\"\n",
    "CDL_DIR   = f\"{DATA_DIR}/cdl\"\n",
    "SOIL_DIR  = f\"{DATA_DIR}/soils\"\n",
    "NDVI_DIR  = f\"{DATA_DIR}/ndvi\"\n",
    "AUX_DIR   = f\"{DATA_DIR}/aux\"\n",
    "\n",
    "# ðŸ” Credentials (fill in as needed)\n",
    "# NASA Earthdata (GLDAS). You should also create a ~/.netrc file for requests to use.\n",
    "EARTHDATA_USERNAME = \"YOUR_USERNAME\"\n",
    "EARTHDATA_PASSWORD = \"YOUR_PASSWORD\"\n",
    "\n",
    "# NASS QuickStats\n",
    "NASS_API_KEY = \"YOUR_QUICKSTATS_API_KEY\"\n",
    "\n",
    "# AppEEARS (optional for MODIS NDVI). If not provided, the Sentinel-2 fallback is used.\n",
    "APPEEARS_TOKEN = None  # e.g., \"eyJhbGciOi...\"\n",
    "\n",
    "# Diesel / Ethanol / Basis: we include example endpoints & parsers. Some require free registration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07fed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, io, sys, json, math, zipfile, textwrap, datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box, Point, Polygon\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "\n",
    "import requests\n",
    "\n",
    "# Ensure cache dirs\n",
    "for d in [DATA_DIR, PRISM_DIR, GLDAS_DIR, USDM_DIR, CDL_DIR, SOIL_DIR, NDVI_DIR, AUX_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def month_range(start_year, end_year):\n",
    "    dates = []\n",
    "    for y in range(start_year, end_year+1):\n",
    "        for m in range(1, 13):\n",
    "            dates.append(dt.date(y, m, 1))\n",
    "    return dates\n",
    "\n",
    "def end_of_month(d: dt.date) -> dt.date:\n",
    "    if d.month == 12:\n",
    "        return dt.date(d.year, 12, 31)\n",
    "    next_m = dt.date(d.year, d.month+1, 1)\n",
    "    return next_m - dt.timedelta(days=1)\n",
    "\n",
    "def ensure_state_counties_mn() -> gpd.GeoDataFrame:\n",
    "    \"\"\"Download and return Minnesota county polygons from TIGER/Line (Census).\"\"\"\n",
    "    url = \"https://www2.census.gov/geo/tiger/TIGER2023/COUNTY/tl_2023_us_county.zip\"\n",
    "    zpath = os.path.join(AUX_DIR, \"tl_2023_us_county.zip\")\n",
    "    if not os.path.exists(zpath):\n",
    "        r = requests.get(url, timeout=120)\n",
    "        r.raise_for_status()\n",
    "        with open(zpath, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "    with zipfile.ZipFile(zpath) as z:\n",
    "        z.extractall(os.path.join(AUX_DIR, \"tl_2023_us_county\"))\n",
    "    g = gpd.read_file(os.path.join(AUX_DIR, \"tl_2023_us_county\", \"tl_2023_us_county.shp\"))\n",
    "    # Minnesota FIPS = 27\n",
    "    g = g[g[\"STATEFP\"] == \"27\"].to_crs(4326)\n",
    "    g[\"fips\"] = g[\"STATEFP\"] + g[\"COUNTYFP\"]\n",
    "    g = g[[\"fips\", \"NAME\", \"geometry\"]].reset_index(drop=True)\n",
    "    return g\n",
    "\n",
    "MN_COUNTIES = ensure_state_counties_mn()\n",
    "MN_BOUNDS = MN_COUNTIES.to_crs(4326).unary_union.bounds\n",
    "MN_COUNTIES.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433f328",
   "metadata": {},
   "source": [
    "\n",
    "## PRISM (Temperature & Precipitation)\n",
    "\n",
    "We pull **monthly** PRISM tmax, tmin, and precipitation grids and compute:\n",
    "- `prism_prcp_mm`, `prism_tmax_c`, `prism_tmin_c`\n",
    "- **Hot-day counts** (>32Â°C or >35Â°C approximations via monthly degree-day proxy)\n",
    "- **VPD proxy** using monthly tmin/tmax\n",
    "\n",
    "> Note: For exact daily hot-day counts and VPD, prefer daily PRISM. Monthly approximations are used here for efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcff5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prism_monthly_url(var: str, year: int, month: int):\n",
    "    # PRISM monthly AN81m products (4km). Zips include .bil rasters.\n",
    "    # var in {\"ppt\", \"tmax\", \"tmin\"}\n",
    "    base = \"https://services.nacse.org/prism/data/public/4km\"\n",
    "    return f\"{base}/{var}/{year}/{month:02d}\"\n",
    "\n",
    "def fetch_prism_monthly(var: str, year: int, month: int) -> Path:\n",
    "    # The PRISM service returns the raw raster file (bil) content.\n",
    "    outdir = Path(PRISM_DIR) / var / f\"{year}\"\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    outfile = outdir / f\"prism_{var}_{year}_{month:02d}.bil\"\n",
    "    hdrfile = outdir / f\"prism_{var}_{year}_{month:02d}.hdr\"\n",
    "    if outfile.exists() and hdrfile.exists():\n",
    "        return outfile\n",
    "\n",
    "    url = prism_monthly_url(var, year, month)\n",
    "    r = requests.get(url, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    # The service returns a binary bil; the header is at .../meta\n",
    "    with open(outfile, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    # fetch hdr\n",
    "    meta_url = url + \"/meta\"\n",
    "    m = requests.get(meta_url, timeout=120)\n",
    "    m.raise_for_status()\n",
    "    # Convert meta JSON to .hdr\n",
    "    md = m.json()\n",
    "    ncols = md[\"ncols\"]; nrows = md[\"nrows\"]\n",
    "    xll = md[\"xllcorner\"]; yll = md[\"yllcorner\"]\n",
    "    cellsize = md[\"cellsize\"]; nodata = md[\"NODATA_value\"]\n",
    "    hdr = f\"\"\"ncols {ncols}\n",
    "nrows {nrows}\n",
    "xllcorner {xll}\n",
    "yllcorner {yll}\n",
    "cellsize {cellsize}\n",
    "NODATA_value {nodata}\n",
    "byteorder LSBFIRST\n",
    "layout bil\n",
    "bands 1\n",
    "\"\"\"\n",
    "    with open(hdrfile, \"w\") as f:\n",
    "        f.write(hdr)\n",
    "    return outfile\n",
    "\n",
    "def read_prism_to_xr(path_bil: Path):\n",
    "    # Read BIL with rasterio -> DataArray w/ rioxarray\n",
    "    with rasterio.open(path_bil) as src:\n",
    "        da = rxr.open_rasterio(src)\n",
    "    # da dims: band, y, x\n",
    "    # Add CRS\n",
    "    da = da.squeeze(\"band\", drop=True)\n",
    "    da.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "    return da\n",
    "\n",
    "def area_weighted_county_mean(da: xr.DataArray, counties_gdf: gpd.GeoDataFrame) -> pd.DataFrame:\n",
    "    results = []\n",
    "    for _, row in counties_gdf.iterrows():\n",
    "        geom = row.geometry.__geo_interface__\n",
    "        try:\n",
    "            clipped = da.rio.clip([geom], counties_gdf.crs, drop=True)\n",
    "        except Exception:\n",
    "            continue\n",
    "        # area-weighted mean (approx via cos(lat) weight)\n",
    "        vals = clipped.values\n",
    "        # mask nodata\n",
    "        mask_valid = np.isfinite(vals)\n",
    "        if not mask_valid.any():\n",
    "            mean_val = np.nan\n",
    "        else:\n",
    "            # latitude weights\n",
    "            yy, xx = np.meshgrid(np.arange(clipped.shape[0]), np.arange(clipped.shape[1]), indexing=\"ij\")\n",
    "            # approximate lat from coordinates\n",
    "            lat = clipped.y.values\n",
    "            w = np.cos(np.deg2rad(lat))[:, None] * np.ones_like(vals)\n",
    "            mean_val = np.average(vals[mask_valid], weights=w[mask_valid])\n",
    "        results.append({\"fips\": row.fips, \"value\": float(mean_val)})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def build_prism_table(start_year=START_YEAR, end_year=END_YEAR):\n",
    "    rows = []\n",
    "    for y in tqdm(range(start_year, end_year+1), desc=\"PRISM years\"):\n",
    "        for m in range(1,13):\n",
    "            for var in [\"ppt\", \"tmax\", \"tmin\"]:\n",
    "                bil = fetch_prism_monthly(var, y, m)\n",
    "                da = read_prism_to_xr(bil)\n",
    "                df = area_weighted_county_mean(da, MN_COUNTIES)\n",
    "                df[\"year\"] = y; df[\"month\"] = m; df[\"var\"] = var\n",
    "                rows.append(df)\n",
    "    prism = pd.concat(rows, ignore_index=True)\n",
    "    prism_pvt = prism.pivot_table(index=[\"fips\",\"year\",\"month\"], columns=\"var\", values=\"value\").reset_index()\n",
    "    # Convert units\n",
    "    # PRISM ppt is mm for AN81m service; tmax/tmin in Celsius (service returns Â°C for meta-backed products).\n",
    "    prism_pvt = prism_pvt.rename(columns={\"ppt\":\"prism_prcp_mm\",\"tmax\":\"prism_tmax_c\",\"tmin\":\"prism_tmin_c\"})\n",
    "    return prism_pvt\n",
    "\n",
    "prism_df = build_prism_table()\n",
    "prism_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd478e02",
   "metadata": {},
   "source": [
    "\n",
    "## GLDAS (Noah 0.25Â°, monthly)\n",
    "\n",
    "We fetch monthly GLDAS variables and aggregate to counties:\n",
    "- `gldas_soil_moisture_0_10cm`, `gldas_et_mm`, `gldas_rn_wm2`\n",
    "\n",
    "> âš ï¸ Requires **NASA Earthdata** login via `.netrc` or session cookies. See: https://disc.gsfc.nasa.gov/earthdata-login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce09b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "def gldas_monthly_urls(year: int, month: int) -> Dict[str, str]:\n",
    "    # GLDAS Noah v2.1 0.25 deg monthly (netCDF)\n",
    "    base = \"https://hydro1.gesdisc.eosdis.nasa.gov/data/GLDAS/GLDAS_NOAH025_M.2.1\"\n",
    "    ydir = f\"{year:04d}\"\n",
    "    fname = f\"GLDAS_NOAH025_M.A{year:04d}{month:02d}.021.nc4\"\n",
    "    url = f\"{base}/{ydir}/{fname}\"\n",
    "    return {\"nc\": url}\n",
    "\n",
    "def fetch_gldas_month(year: int, month: int) -> Path:\n",
    "    outdir = Path(GLDAS_DIR) / f\"{year}\"\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    path = outdir / f\"GLDAS_NOAH025_M.A{year:04d}{month:02d}.021.nc4\"\n",
    "    if path.exists():\n",
    "        return path\n",
    "    urls = gldas_monthly_urls(year, month)\n",
    "    with requests.Session() as s:\n",
    "        s.auth = HTTPBasicAuth(EARTHDATA_USERNAME, EARTHDATA_PASSWORD)\n",
    "        r = s.get(urls[\"nc\"], stream=True, timeout=180)\n",
    "        r.raise_for_status()\n",
    "        with open(path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1<<20):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    return path\n",
    "\n",
    "def build_gldas_table(start_year=START_YEAR, end_year=END_YEAR):\n",
    "    rows = []\n",
    "    for y in tqdm(range(start_year, end_year+1), desc=\"GLDAS years\"):\n",
    "        for m in range(1,13):\n",
    "            nc = fetch_gldas_month(y, m)\n",
    "            ds = xr.open_dataset(nc)\n",
    "            # Variables of interest (names may differ by version; adjust if needed)\n",
    "            # Soil moisture (0-10cm): Soilm_tavg (kg/m2) for layer 0-10cm ~ mm of water\n",
    "            # Evapotranspiration: Evap_tavg (kg/m2/s); convert to mm/month\n",
    "            # Net radiation: Rn_tavg (W/m2) average over month\n",
    "            var_map = {\n",
    "                \"SoilMoi0_10cm_inst\": [\"SoilMoi0_10cm_inst\",\"Soilm_tavg\",\"RootMoist_inst\"],\n",
    "                \"Evap_tavg\": [\"Evap_tavg\"],\n",
    "                \"Rn_tavg\": [\"Rn_tavg\"]\n",
    "            }\n",
    "            # pick first available\n",
    "            def pick(ds, names):\n",
    "                for n in names:\n",
    "                    if n in ds:\n",
    "                        return ds[n]\n",
    "                raise KeyError(f\"None of {names} found in GLDAS file for {y}-{m}\")\n",
    "\n",
    "            # Soil moisture 0-10cm (mm water equiv if kg/m2 ~ mm)\n",
    "            sm = pick(ds, var_map[\"SoilMoi0_10cm_inst\"])\n",
    "            # ET (kg/m2/s) -> mm/month: multiply by seconds in month\n",
    "            et = pick(ds, var_map[\"Evap_tavg\"])\n",
    "            rn = pick(ds, var_map[\"Rn_tavg\"])\n",
    "\n",
    "            # Subset to MN bbox to speed up\n",
    "            lat_cond = (ds[\"lat\"] >= MN_BOUNDS[1]-1) & (ds[\"lat\"] <= MN_BOUNDS[3]+1)\n",
    "            lon_cond = (ds[\"lon\"] >= MN_BOUNDS[0]-1) & (ds[\"lon\"] <= MN_BOUNDS[2]+1)\n",
    "            sm = sm.sel(lat=lat_cond, lon=lon_cond)\n",
    "            et = et.sel(lat=lat_cond, lon=lon_cond)\n",
    "            rn = rn.sel(lat=lat_cond, lon=lon_cond)\n",
    "\n",
    "            # To rioxarray for clipping\n",
    "            sm_da = sm.rename(\"sm\").rio.write_crs(4326)\n",
    "            et_da = et.rename(\"et\").rio.write_crs(4326)\n",
    "            rn_da = rn.rename(\"rn\").rio.write_crs(4326)\n",
    "\n",
    "            # Convert ET to mm/month\n",
    "            days = (end_of_month(dt.date(y,m,1)) - dt.date(y,m,1)).days + 1\n",
    "            seconds = days * 24 * 3600\n",
    "            et_mm = et_da * seconds  # kg/m2/s ~ mm/s * seconds -> mm\n",
    "\n",
    "            # County aggregation\n",
    "            def county_mean(da):\n",
    "                res = []\n",
    "                # flip lat if needed\n",
    "                if np.all(np.diff(da[\"lat\"].values) < 0):\n",
    "                    da = da.sel(lat=slice(None,None,-1))\n",
    "                # build affine via rioxarray\n",
    "                da = da.rio.write_crs(4326)\n",
    "                for _, row in MN_COUNTIES.iterrows():\n",
    "                    geom = [row.geometry.__geo_interface__]\n",
    "                    try:\n",
    "                        cl = da.rio.clip(geom, MN_COUNTIES.crs, drop=True)\n",
    "                        val = float(np.nanmean(cl.values))\n",
    "                    except Exception:\n",
    "                        val = np.nan\n",
    "                    res.append({\"fips\": row.fips, \"value\": val})\n",
    "                return pd.DataFrame(res)\n",
    "\n",
    "            sm_df = county_mean(sm_da)\n",
    "            et_df = county_mean(et_mm)\n",
    "            rn_df = county_mean(rn_da)\n",
    "\n",
    "            merged = sm_df.merge(et_df, on=\"fips\", suffixes=(\"_sm\",\"_et\")).merge(rn_df, on=\"fips\")\n",
    "            merged = merged.rename(columns={\n",
    "                \"value_sm\":\"gldas_soil_moisture_0_10cm\",\n",
    "                \"value_et\":\"gldas_et_mm\",\n",
    "                \"value\":\"gldas_rn_wm2\"\n",
    "            })\n",
    "            merged[\"year\"]=y; merged[\"month\"]=m\n",
    "            rows.append(merged)\n",
    "            ds.close()\n",
    "    gldas_df = pd.concat(rows, ignore_index=True)\n",
    "    return gldas_df\n",
    "\n",
    "gldas_df = build_gldas_table()\n",
    "gldas_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c254ca",
   "metadata": {},
   "source": [
    "\n",
    "## U.S. Drought Monitor (USDM)\n",
    "\n",
    "We compute:\n",
    "- `usdm_pct_D2plus`: percent of county area in D2â€“D4 during the month\n",
    "- `usdm_days_D2plus`: number of **days** in D2+ within the month\n",
    "\n",
    "We download weekly shapefiles and aggregate by county and by day-count per month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82b213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def usdm_weekly_zip_url(date_obj: dt.date) -> str:\n",
    "    # Weekly product published on Thursdays; filename uses YYYYMMDD\n",
    "    dstr = date_obj.strftime(\"%Y%m%d\")\n",
    "    return f\"https://droughtmonitor.unl.edu/data/shapefiles_m/USDM_{dstr}_M.zip\"\n",
    "\n",
    "def nearest_usdm_thursday(d: dt.date) -> dt.date:\n",
    "    # USDM publication day = Thursday\n",
    "    # Find the Thursday of that week (or nearest previous)\n",
    "    return d - dt.timedelta(days=(d.weekday()-3)%7)\n",
    "\n",
    "def fetch_usdm_weekly(d: dt.date) -> Path:\n",
    "    th = nearest_usdm_thursday(d)\n",
    "    zname = f\"USDM_{th.strftime('%Y%m%d')}_M.zip\"\n",
    "    zpath = Path(USDM_DIR) / zname\n",
    "    if not zpath.exists():\n",
    "        url = usdm_weekly_zip_url(th)\n",
    "        r = requests.get(url, timeout=120)\n",
    "        if r.status_code == 404:\n",
    "            # some early years not available in this path; you may need alternative archive\n",
    "            return None\n",
    "        r.raise_for_status()\n",
    "        with open(zpath, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        with zipfile.ZipFile(zpath) as z:\n",
    "            z.extractall(Path(USDM_DIR) / zname.replace(\".zip\",\"\"))\n",
    "    return zpath\n",
    "\n",
    "def build_usdm_table(start_year=START_YEAR, end_year=END_YEAR):\n",
    "    rows = []\n",
    "    for y in tqdm(range(start_year, end_year+1), desc=\"USDM years\"):\n",
    "        for m in range(1,13):\n",
    "            # collect all Thursdays that fall inside this month\n",
    "            first = dt.date(y,m,1); last = end_of_month(first)\n",
    "            d = first\n",
    "            weekly_polys = []\n",
    "            while d <= last:\n",
    "                z = fetch_usdm_weekly(d)\n",
    "                if z is not None:\n",
    "                    folder = Path(str(z).replace(\".zip\",\"\"))\n",
    "                    # shapefile name pattern: USDM_YYYYMMDD_M.shp\n",
    "                    shp = None\n",
    "                    for f in os.listdir(folder):\n",
    "                        if f.endswith(\".shp\"):\n",
    "                            shp = os.path.join(folder, f)\n",
    "                            break\n",
    "                    if shp:\n",
    "                        g = gpd.read_file(shp).to_crs(4326)\n",
    "                        # code column may be \"DM\" with values D0..D4; map to integer\n",
    "                        g[\"level\"] = g[\"DM\"].map({\"D0\":0,\"D1\":1,\"D2\":2,\"D3\":3,\"D4\":4}).fillna(-1)\n",
    "                        weekly_polys.append(g[[\"level\",\"geometry\"]])\n",
    "                d += dt.timedelta(days=7)\n",
    "            if not weekly_polys:\n",
    "                continue\n",
    "            # Approximate daily coverage by assuming weekly map holds for 7 days\n",
    "            # Intersect with counties, compute area share in D2+\n",
    "            month_res = []\n",
    "            for _, crow in MN_COUNTIES.iterrows():\n",
    "                cgeom = gpd.GeoSeries([crow.geometry], crs=4326)\n",
    "                total_area = cgeom.to_crs(3857).area.values[0]  # m^2\n",
    "                days_D2p = 0\n",
    "                area_share_accum = []\n",
    "                for wg in weekly_polys:\n",
    "                    inter = gpd.overlay(gpd.GeoDataFrame(geometry=cgeom), wg, how=\"intersection\")\n",
    "                    if inter.empty:\n",
    "                        area_share_accum.append(0.0)\n",
    "                        continue\n",
    "                    inter = inter[inter[\"level\"]>=2]\n",
    "                    if inter.empty:\n",
    "                        area_share_accum.append(0.0)\n",
    "                        continue\n",
    "                    inter_area = inter.to_crs(3857).area.sum()\n",
    "                    pct = float(inter_area/total_area*100.0)\n",
    "                    area_share_accum.append(pct)\n",
    "                    if pct>0:\n",
    "                        days_D2p += 7  # 7-day block\n",
    "                usdm_pct = np.mean(area_share_accum) if area_share_accum else 0.0\n",
    "                month_res.append({\n",
    "                    \"fips\": crow.fips, \"year\": y, \"month\": m,\n",
    "                    \"usdm_pct_D2plus\": usdm_pct,\n",
    "                    \"usdm_days_D2plus\": days_D2p\n",
    "                })\n",
    "            rows.append(pd.DataFrame(month_res))\n",
    "    usdm_df = pd.concat(rows, ignore_index=True)\n",
    "    return usdm_df\n",
    "\n",
    "usdm_df = build_usdm_table()\n",
    "usdm_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b71953",
   "metadata": {},
   "source": [
    "\n",
    "## ENSO â€” Oceanic NiÃ±o Index (ONI)\n",
    "\n",
    "We ingest NOAA's ONI table and join monthly with a 1â€“3 month lead option.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_oni_table() -> pd.DataFrame:\n",
    "    url = \"https://www.cpc.ncep.noaa.gov/data/indices/oni.ascii.txt\"\n",
    "    r = requests.get(url, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    # Parse simple whitespace table\n",
    "    lines = [ln for ln in r.text.splitlines() if ln.strip() and not ln.startswith(\"YR\")]\n",
    "    records = []\n",
    "    for ln in lines:\n",
    "        parts = ln.split()\n",
    "        yr = int(parts[0]); vals = parts[1:13]\n",
    "        for i, v in enumerate(vals, start=1):\n",
    "            try:\n",
    "                oni = float(v)\n",
    "            except:\n",
    "                oni = np.nan\n",
    "            records.append({\"year\": yr, \"month\": i, \"oni\": oni})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "oni_df = fetch_oni_table()\n",
    "oni_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f511284e",
   "metadata": {},
   "source": [
    "\n",
    "## Cropland Data Layer (CDL) â€” Corn Fraction\n",
    "\n",
    "We compute the fraction of each county planted to **corn** using CDL (2008â€“present).  \n",
    "This uses the public AWS endpoint maintained by USDA/NASS (GeoTIFF per year/state or CONUS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dad996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_cdl_year(year: int) -> Path:\n",
    "    # Use CONUS GeoTIFF (might be large). Alternatively, pull per-state tiles if available.\n",
    "    url = f\"https://www.nass.usda.gov/Research_and_Science/Cropland/Release/datasets/CDL_{year}_v6.tif\"\n",
    "    out = Path(CDL_DIR)/f\"CDL_{year}.tif\"\n",
    "    if out.exists():\n",
    "        return out\n",
    "    r = requests.get(url, timeout=300)\n",
    "    if r.status_code == 404:\n",
    "        # older naming\n",
    "        url = f\"https://www.nass.usda.gov/Research_and_Science/Cropland/Release/datasets/USDA_CDL_{year}.tif\"\n",
    "        r = requests.get(url, timeout=300)\n",
    "    r.raise_for_status()\n",
    "    with open(out, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return out\n",
    "\n",
    "CORN_CODES = {1, 3}  # 1=Corn (yellow), 3=Sweet Corn (check legend by year)\n",
    "\n",
    "def county_corn_fraction_from_cdl(tif_path: Path, counties: gpd.GeoDataFrame) -> pd.DataFrame:\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        res = []\n",
    "        for _, row in counties.iterrows():\n",
    "            geom = [row.geometry.__geo_interface__]\n",
    "            try:\n",
    "                out_image, out_transform = mask(src, geom, crop=True)\n",
    "            except Exception:\n",
    "                res.append({\"fips\": row.fips, \"corn_frac\": np.nan})\n",
    "                continue\n",
    "            data = out_image[0]\n",
    "            if data.size == 0:\n",
    "                res.append({\"fips\": row.fips, \"corn_frac\": np.nan})\n",
    "                continue\n",
    "            valid = data > 0\n",
    "            if valid.sum() == 0:\n",
    "                res.append({\"fips\": row.fips, \"corn_frac\": 0.0})\n",
    "                continue\n",
    "            corn = np.isin(data, list(CORN_CODES))\n",
    "            frac = float(corn.sum()/valid.sum())\n",
    "            res.append({\"fips\": row.fips, \"corn_frac\": frac})\n",
    "        return pd.DataFrame(res)\n",
    "\n",
    "def build_cdl_table(start_year=2008, end_year=END_YEAR):\n",
    "    rows = []\n",
    "    for y in tqdm(range(start_year, min(end_year, dt.date.today().year)+1), desc=\"CDL years\"):\n",
    "        tif = fetch_cdl_year(y)\n",
    "        df = county_corn_fraction_from_cdl(tif, MN_COUNTIES)\n",
    "        df[\"year\"]=y; df[\"cdl_corn_fraction\"]=df.pop(\"corn_frac\")\n",
    "        rows.append(df)\n",
    "    return pd.concat(rows, ignore_index=True)\n",
    "\n",
    "cdl_df = build_cdl_table()\n",
    "cdl_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece1084",
   "metadata": {},
   "source": [
    "\n",
    "## gSSURGO â€” Soils (AWC, drainage, OM)\n",
    "\n",
    "We query the Soil Data Access (SDA) API to compute county-level aggregates for:\n",
    "- `gssurgo_awc` (available water capacity in top 1 m)\n",
    "- `gssurgo_drainage_idx` (ordinal drainage class)\n",
    "- (Optional) `om` (organic matter) as a cross-check\n",
    "\n",
    "> This uses an SDA SQL query and intersects with county boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c70f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sda_query(sql: str) -> pd.DataFrame:\n",
    "    url = \"https://sdmdataaccess.sc.egov.usda.gov/tabular/post.rest\"\n",
    "    headers = {\"Content-Type\":\"application/json\"}\n",
    "    payload = {\"query\": sql}\n",
    "    r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=180)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "    if \"Table\" not in js:\n",
    "        raise RuntimeError(f\"SDA response missing 'Table': {js}\")\n",
    "    return pd.DataFrame(js[\"Table\"])\n",
    "\n",
    "# We'll pull mapunits that intersect Minnesota and compute AWC from component/horizon tables.\n",
    "SQL = '''\n",
    "SELECT mu.mukey, mu.musym, mu.muname, areasymbol, areaname, mu.acres\n",
    "FROM legend lg\n",
    "JOIN mapunit mu ON mu.lkey = lg.lkey\n",
    "WHERE areasymbol LIKE 'MN%'\n",
    "'''\n",
    "try:\n",
    "    mu = sda_query(SQL)\n",
    "    mu.head()\n",
    "except Exception as e:\n",
    "    print(\"SDA query failed; check internet access.\", e)\n",
    "\n",
    "# For brevity, we show structure; full AWC computation typically aggregates from CHORIZON by thickness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e04904",
   "metadata": {},
   "source": [
    "\n",
    "## NDVI (MODIS or Sentinel-2)\n",
    "\n",
    "Two options:\n",
    "\n",
    "1) **MODIS MOD13A2 (16-day, 1 km)** via AppEEARS API (recommended for long record).  \n",
    "2) **Sentinel-2** via STAC search (10 m) for recent years.\n",
    "\n",
    "We then compute: `ndvi_mean`, `ndvi_peak`, `ndvi_peak_doy`, `ndvi_anom` per month/county.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c63fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def placeholder_ndvi_builder():\n",
    "    # Implement AppEEARS task creation with APPEEARS_TOKEN or use Sentinel-2 STAC search.\n",
    "    # After download, mosaic to monthly composites over MN and aggregate to counties similar to PRISM/GLDAS.\n",
    "    # For now, we create a stub DataFrame to allow the pipeline to run.\n",
    "    rows = []\n",
    "    for y in range(START_YEAR, END_YEAR+1):\n",
    "        for m in range(1,13):\n",
    "            for f in MN_COUNTIES[\"fips\"]:\n",
    "                rows.append({\n",
    "                    \"fips\": f, \"year\": y, \"month\": m,\n",
    "                    \"ndvi_mean\": np.nan, \"ndvi_peak\": np.nan,\n",
    "                    \"ndvi_peak_doy\": np.nan, \"ndvi_anom\": np.nan\n",
    "                })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "ndvi_df = placeholder_ndvi_builder()\n",
    "ndvi_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d77b6ba",
   "metadata": {},
   "source": [
    "\n",
    "## Markets & Logistics Proxies\n",
    "\n",
    "- **Diesel** (EIA weekly) â†’ monthly mean around planting/harvest.  \n",
    "- **Ethanol plants** (EIA list) â†’ compute `dist_km_ethanol` from county centroid to nearest plant.  \n",
    "- **Basis** (USDA AMS bids; optional) â†’ join nearest terminal to county; use Mayâ€“June mean.\n",
    "\n",
    "> We include diesel and ethanol distance here. Basis collection varies by terminal and may require custom scrapers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd0430",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from math import radians, sin, cos, asin, sqrt\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    dlat = radians(lat2-lat1); dlon = radians(lon2-lon1)\n",
    "    a = sin(dlat/2)**2 + cos(radians(lat1))*cos(radians(lat2))*sin(dlon/2)**2\n",
    "    return 2*R*asin(sqrt(a))\n",
    "\n",
    "def fetch_eia_ethanol_plants() -> pd.DataFrame:\n",
    "    # Example CSV (public snapshot). Replace with EIA authoritative list if available.\n",
    "    # For now, we provide a small MN-centric list as a starter.\n",
    "    plants = [\n",
    "        {\"name\":\"Guardian Energy - Janesville, MN\",\"lat\":44.12,\"lon\":-93.71},\n",
    "        {\"name\":\"POET - Preston, MN\",\"lat\":43.67,\"lon\":-92.09},\n",
    "        {\"name\":\"Green Plains - Fairmont, MN\",\"lat\":43.63,\"lon\":-94.46},\n",
    "        {\"name\":\"Bushmills Ethanol - Atwater, MN\",\"lat\":45.13,\"lon\":-94.79},\n",
    "        {\"name\":\"POET - Glenville, MN\",\"lat\":43.56,\"lon\":-93.34},\n",
    "    ]\n",
    "    return pd.DataFrame(plants)\n",
    "\n",
    "def compute_ethanol_distance():\n",
    "    plants = fetch_eia_ethanol_plants()\n",
    "    rows = []\n",
    "    centroids = MN_COUNTIES.copy()\n",
    "    centroids[\"centroid\"] = centroids.geometry.centroid\n",
    "    for _, r in centroids.iterrows():\n",
    "        clat = r.centroid.y; clon = r.centroid.x\n",
    "        dmin = 1e9; which = None\n",
    "        for _, p in plants.iterrows():\n",
    "            d = haversine_km(clat, clon, p.lat, p.lon)\n",
    "            if d < dmin:\n",
    "                dmin = d; which = p[\"name\"]\n",
    "        rows.append({\"fips\": r.fips, \"dist_km_ethanol\": dmin, \"nearest_ethanol\": which})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "ethanol_df = compute_ethanol_distance()\n",
    "ethanol_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edbb97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_eia_diesel_weekly() -> pd.DataFrame:\n",
    "    # EIA weekly U.S. No. 2 Diesel Retail Prices (Dollars per Gallon) â€” national series\n",
    "    # CSV download:\n",
    "    url = \"https://www.eia.gov/dnav/pet/hist_xls/RWTCd.htm\"  # Placeholder; consider EIA API for JSON with API key.\n",
    "    # For illustration, we will create a stub series (you should replace with EIA API call).\n",
    "    dates = pd.date_range(\"1994-01-03\",\"2025-10-20\", freq=\"W-MON\")\n",
    "    vals = np.clip(2.5 + 0.5*np.sin(np.linspace(0, 50, len(dates))), 1.5, 6.0)\n",
    "    return pd.DataFrame({\"date\":dates, \"diesel_usd_gal\": vals})\n",
    "\n",
    "def diesel_monthly_mean(diesel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    diesel_df[\"year\"] = diesel_df[\"date\"].dt.year\n",
    "    diesel_df[\"month\"] = diesel_df[\"date\"].dt.month\n",
    "    out = diesel_df.groupby([\"year\",\"month\"]).agg(diesel_usd_gal=(\"diesel_usd_gal\",\"mean\")).reset_index()\n",
    "    return out\n",
    "\n",
    "diesel_df = diesel_monthly_mean(fetch_eia_diesel_weekly())\n",
    "diesel_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c5ffd",
   "metadata": {},
   "source": [
    "\n",
    "## NASS QuickStats â€” Targets\n",
    "\n",
    "Pull county-level **yield (bu/acre)** and **harvested acres** for corn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995543c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def quickstats_query(params: Dict) -> pd.DataFrame:\n",
    "    base = \"https://quickstats.nass.usda.gov/api/api_GET/\"\n",
    "    params[\"key\"] = NASS_API_KEY\n",
    "    r = requests.get(base, params=params, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "    if \"data\" not in js:\n",
    "        raise RuntimeError(f\"QuickStats response missing data: {js}\")\n",
    "    return pd.DataFrame(js[\"data\"])\n",
    "\n",
    "def fetch_mn_corn_targets(start_year=START_YEAR, end_year=END_YEAR):\n",
    "    common = {\n",
    "        \"source_desc\": \"SURVEY\",\n",
    "        \"sector_desc\": \"CROPS\",\n",
    "        \"group_desc\": \"FIELD CROPS\",\n",
    "        \"commodity_desc\": \"CORN\",\n",
    "        \"agg_level_desc\": \"COUNTY\",\n",
    "        \"state_alpha\": \"MN\",\n",
    "        \"statisticcat_desc\": \"YIELD\"\n",
    "    }\n",
    "    yld = quickstats_query({**common, \"unit_desc\":\"BU / ACRE\", \"year__GE\": start_year, \"year__LE\": end_year})\n",
    "    yld = yld[[\"year\",\"county_code\",\"value\"]].rename(columns={\"county_code\":\"countyfp\",\"value\":\"target_yield_bu_ac\"})\n",
    "    yld[\"target_yield_bu_ac\"] = pd.to_numeric(yld[\"target_yield_bu_ac\"], errors=\"coerce\")\n",
    "    harv = quickstats_query({\n",
    "        **common,\n",
    "        \"statisticcat_desc\":\"AREA HARVESTED\",\n",
    "        \"unit_desc\":\"ACRES\",\n",
    "        \"year__GE\": start_year, \"year__LE\": end_year\n",
    "    })\n",
    "    harv = harv[[\"year\",\"county_code\",\"value\"]].rename(columns={\"county_code\":\"countyfp\",\"value\":\"target_harvested_acres\"})\n",
    "    harv[\"target_harvested_acres\"] = pd.to_numeric(harv[\"target_harvested_acres\"], errors=\"coerce\")\n",
    "    # Join to FIPS (state 27)\n",
    "    yld[\"fips\"] = \"27\" + yld[\"countyfp\"].str.zfill(3)\n",
    "    harv[\"fips\"] = \"27\" + harv[\"countyfp\"].str.zfill(3)\n",
    "    tgt = yld.merge(harv[[\"year\",\"fips\",\"target_harvested_acres\"]], on=[\"year\",\"fips\"], how=\"outer\")\n",
    "    # add month placeholder for joining (annual target applies to all months for feature alignment; we'll reduce at horizon)\n",
    "    tgt[\"month\"] = 12\n",
    "    return tgt\n",
    "\n",
    "targets_df = fetch_mn_corn_targets()\n",
    "targets_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1d7644",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Engineering & Merge to Minimal Schema\n",
    "\n",
    "We compute:\n",
    "- `heat_days_35c` (approximate from PRISM monthly Tmax via exceedance proxy)\n",
    "- `vpd_proxy` (simple Clausiusâ€“Clapeyron-based monthly proxy)\n",
    "- `moisture_deficit = prism_prcp_mm - gldas_et_mm`\n",
    "- Label horizons and keep only rows up to each horizon month per year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec90661",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vpd_proxy_from_tmin_tmax_c(tmin_c, tmax_c):\n",
    "    # Simple monthly mean VPD proxy using saturation vapor pressure at tmax minus at tmin\n",
    "    def esat(t):\n",
    "        return 0.6108 * np.exp((17.27*t)/(t+237.3))  # kPa\n",
    "    es_max = esat(tmax_c)\n",
    "    es_min = esat(tmin_c)\n",
    "    return np.maximum(es_max - es_min, 0.0)\n",
    "\n",
    "def heatday_proxy_from_monthly_tmax(tmax_c, threshold_c=35.0):\n",
    "    # Approximate # of hot days in month by assuming a normal diurnal distribution around tmax.\n",
    "    # Very rough: days_in_month * sigmoid((tmax - thr)/2)\n",
    "    days = 30.0\n",
    "    return days * (1/(1+np.exp(-(tmax_c - threshold_c)/2)))\n",
    "\n",
    "def assemble_minimal_schema(prism_df, gldas_df, usdm_df, oni_df, cdl_df, ndvi_df, diesel_df, ethanol_df, targets_df):\n",
    "    # Merge monthly\n",
    "    df = prism_df.merge(gldas_df, on=[\"fips\",\"year\",\"month\"], how=\"left\")\\\n",
    "                  .merge(usdm_df, on=[\"fips\",\"year\",\"month\"], how=\"left\")\\\n",
    "                  .merge(oni_df, on=[\"year\",\"month\"], how=\"left\")\\\n",
    "                  .merge(cdl_df, on=[\"fips\",\"year\"], how=\"left\")\\\n",
    "                  .merge(ndvi_df, on=[\"fips\",\"year\",\"month\"], how=\"left\")\\\n",
    "                  .merge(diesel_df, on=[\"year\",\"month\"], how=\"left\")\\\n",
    "                  .merge(ethanol_df, on=[\"fips\"], how=\"left\")\n",
    "    # Engineering\n",
    "    df[\"vpd_proxy\"] = vpd_proxy_from_tmin_tmax_c(df[\"prism_tmin_c\"], df[\"prism_tmax_c\"])\n",
    "    df[\"heat_days_35c\"] = heatday_proxy_from_monthly_tmax(df[\"prism_tmax_c\"], 35.0)\n",
    "    df[\"moisture_deficit\"] = df[\"prism_prcp_mm\"] - df[\"gldas_et_mm\"]\n",
    "    # Add horizon tags by picking rows month <= horizon month, then marking last row of horizon per year\n",
    "    out_rows = []\n",
    "    for label, spec in HORIZONS.items():\n",
    "        hmon = spec[\"month\"]\n",
    "        sub = df[df[\"month\"] <= hmon].copy()\n",
    "        sub[\"horizon_tag\"] = label\n",
    "        # Keep only the horizon month rows (end-of-window snapshot)\n",
    "        sub = sub[sub[\"month\"]==hmon]\n",
    "        out_rows.append(sub)\n",
    "    out = pd.concat(out_rows, ignore_index=True)\n",
    "    # Attach targets (annual; aligned at year-level)\n",
    "    out = out.merge(targets_df[[\"fips\",\"year\",\"target_yield_bu_ac\",\"target_harvested_acres\"]], on=[\"fips\",\"year\"], how=\"left\")\n",
    "    # Final column order\n",
    "    cols = [\n",
    "        \"fips\",\"year\",\"month\",\"horizon_tag\",\n",
    "        \"prism_prcp_mm\",\"prism_tmax_c\",\"prism_tmin_c\",\n",
    "        \"gldas_soil_moisture_0_10cm\",\"gldas_et_mm\",\"gldas_rn_wm2\",\n",
    "        \"heat_days_35c\",\"vpd_proxy\",\n",
    "        \"ndvi_mean\",\"ndvi_peak\",\"ndvi_peak_doy\",\"ndvi_anom\",\n",
    "        \"usdm_pct_D2plus\",\"usdm_days_D2plus\",\n",
    "        \"oni\",\n",
    "        \"cdl_corn_fraction\",\n",
    "        \"diesel_usd_gal\",\"dist_km_ethanol\",\n",
    "        \"target_yield_bu_ac\",\"target_harvested_acres\"\n",
    "    ]\n",
    "    out = out.reindex(columns=cols)\n",
    "    return out\n",
    "\n",
    "minimal_df = assemble_minimal_schema(prism_df, gldas_df, usdm_df, oni_df, cdl_df, ndvi_df, diesel_df, ethanol_df, targets_df)\n",
    "minimal_path = \"/mnt/data/mn_minimal_schema.csv\"\n",
    "minimal_df.to_csv(minimal_path, index=False)\n",
    "minimal_df.head(), minimal_path\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
